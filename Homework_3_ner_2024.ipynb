{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss9WR_zbQELO"
   },
   "source": [
    "# Практическое задание 3\n",
    "\n",
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дата выдачи: <span style=\"color:red\">__16 ноября 21:00__</span>.\n",
    "\n",
    "Дедлайн: <span style=\"color:red\">__01 декабря 22:00__</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB3vM2GfQELQ"
   },
   "source": [
    "## Введение\n",
    "\n",
    "### Постановка задачи\n",
    "\n",
    "В этом задании вы будете решать задачу извлечения именованных сущностей (Named Entity Recognition) – одну из самых распространенных в NLP, наряду с задачей текстовой классификации.\n",
    "\n",
    "Данная задача заключается в том, что нужно классифицировать каждое слово / токен на предмет того, является ли оно частью именованной сущности (сущность может состоять из нескольких слов / токенов) или нет.\n",
    "\n",
    "Например, мы хотим извлечь имена и названия организаций. Тогда для текста\n",
    "\n",
    "    Yan    Goodfellow  works  for  Google  Brain\n",
    "\n",
    "модель должна извлечь следующую последовательность:\n",
    "\n",
    "    B-PER  I-PER       O      O    B-ORG   I-ORG\n",
    "\n",
    "где префиксы *B-* и *I-* означают начало и конец именованной сущности, *O* означает слово без тега. Такая префиксная система (*BIO*-разметка) введена, чтобы различать последовательные именованные сущности одного типа.\n",
    "Существуют и другие типы разметок, например *BILUO*, но в рамках данного практического задания сфокусируемся имеено на *BIO*.\n",
    "\n",
    "Решать NER задачу мы будем на датасете CoNLL-2003 с использованием рекуррентных сетей и моделей на базе архитектуры Transformer. Датасет CoNLL-2003 представлен в виде разметки **BIO**, где лейбл:\n",
    "- *B-{label}* - начало сущности *{label}*;\n",
    "- *I-{label}* - продолжение сущности *{label}*;\n",
    "- *O* - отсутсвие сущности.\n",
    "\n",
    "Здесь в качестве сущности *{label}* может выступать имя, географическое название или какой-то другой тип собственных имён. Подробнее с разметками можно ознакомится во вспомогательном ноутбуке.\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Основные библиотеки:\n",
    " - [PyTorch](https://pytorch.org/)\n",
    " - [Transformers](https://github.com/huggingface/transformers)\n",
    "\n",
    "### Данные\n",
    "\n",
    "Данные лежат в архиве, который состоит из:\n",
    "\n",
    "- *train.tsv* - обучающая выборка. В каждой строке записаны: <слово / токен>, <тэг слова / токена>\n",
    "\n",
    "- *valid.tsv* - валидационная выборка, которую можно использовать для подбора гиперпарамеров и замеров качества. Имеет идентичную с train.tsv структуру.\n",
    "\n",
    "- *test.tsv* - тестовая выборка, по которой оценивается итоговое качество. Имеет идентичную с train.tsv структуру.\n",
    "\n",
    "Скачать данные можно здесь: [ссылка](https://github.com/valerapon/msu_task_3_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EpTwu2i2qRrg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.5\n",
      "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [33 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\n1117\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "      main()\n",
      "    File \"C:\\Users\\n1117\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\n1117\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 112, in get_requires_for_build_wheel\n",
      "      backend = _build_backend()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\n1117\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 77, in _build_backend\n",
      "      obj = import_module(mod_path)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\n1117\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "      return _bootstrap._gcd_import(name[level:], package, level)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "    File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "    File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "    File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "    File \"C:\\Users\\n1117\\AppData\\Local\\Temp\\pip-build-env-ilpxzyv1\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
      "      import setuptools.version\n",
      "    File \"C:\\Users\\n1117\\AppData\\Local\\Temp\\pip-build-env-ilpxzyv1\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
      "      import pkg_resources\n",
      "    File \"C:\\Users\\n1117\\AppData\\Local\\Temp\\pip-build-env-ilpxzyv1\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
      "      register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.23.5 scikit-learn==1.2.2 tensorboard==2.14.1 torch==2.1.0 tqdm==4.66.1 transformers==4.34.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Thidpb9qQELS"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\n1117\\\\CODES_NOTE\\\\ML_NOTE\\\\HW_7sem\\\\4hw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiDlmbY2QELT"
   },
   "source": [
    "Зафиксируем seed (значение 42) для воспроизводимости результатов (желательно делать **всегда**!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yt3ISg3aQELU"
   },
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set global seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhIg0ZBzQELV"
   },
   "source": [
    "Проинициализируем device (CPU / GPU) на котором будем работать (желательно **GPU**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rboLOv95QELV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k3Nhd3IQELY"
   },
   "source": [
    "## Часть 1. Подготовка данных (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qYjOMuPQELY"
   },
   "source": [
    "Первым делом нам нужно считать данные. Давайте напишем функцию, которая на вход принимает путь до одного из conll-2003 файла и возвращает два списка:\n",
    "- список списков слов / токенов;\n",
    "- список списков тегов, который соответствует собранному списку слов / токенов.\n",
    "\n",
    "Также функция на вход принимает булевую переменную *lowercase*, которая задает чувствительность к регистру. Далее будем всё приводить к нижнему регистру (`lower=True`).\n",
    "\n",
    "P.S. Стоит держать в голове, что в некоторых ситуациях верхний регистр помогает выявлять именованные сущности. Например, у вас нет мощностей, чтобы запускать сложные модели, а задачу решать нужно быстро. В этом случае эвристическое правило: \"Большая буква в слове = именнованная сущность\" может вам помочь. Или у вас есть огромные корпусы данных, которые позволяют сохранять исходное разнообразие слов.\n",
    "\n",
    "**Задание. Реализуйте функцию read_conll2003.** **<font color='red'>(1 балл)</font>**\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка </summary>\n",
    "\n",
    "*Предложения разделены пустой строкой, в конце файла также пустая строка, но не забывайте про символ `\\n`.*\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wQdCfX2OQELZ"
   },
   "outputs": [],
   "source": [
    "def read_conll2003(\n",
    "    path: str,\n",
    "    lower: bool = True,\n",
    ") -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Prepare data in CoNNL like format.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the file (str).\n",
    "        lower:  Reduce text to lowercase (bool).\n",
    "\n",
    "    Returns:\n",
    "        Function returns pair (token_seq, label_seq).\n",
    "        token_seq: The list of lists. Each internal list is\n",
    "            a sentence converted into tokens.\n",
    "        label_seq: The list of lists. All internal lists\n",
    "            contain tags corresponding to tokens from token_seq.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    token_seq: List[List[str]] = []\n",
    "    label_seq: List[List[str]] = []\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(path, \"r\") as text_inp_file:\n",
    "         for line in text_inp_file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                token = parts[0]\n",
    "                tag = parts[1]\n",
    "                if lower:\n",
    "                    token = token.lower()\n",
    "                current_sentence.append(token)\n",
    "                current_tags.append(tag)\n",
    "            else:\n",
    "                if current_sentence:\n",
    "                    token_seq.append(current_sentence)\n",
    "                    label_seq.append(current_tags)\n",
    "                    current_sentence = []\n",
    "                    current_tags = []\n",
    "\n",
    "\n",
    "    return token_seq, label_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYm8xEvFQELb"
   },
   "source": [
    "Считаем все три файла:\n",
    "- *train.tsv*\n",
    "- *valid.tsv*\n",
    "- *test.tsv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-inr1BPgQELb"
   },
   "outputs": [],
   "source": [
    "train_token_seq, train_label_seq = read_conll2003(\"data/train.txt\")\n",
    "valid_token_seq, valid_label_seq = read_conll2003(\"data/valid.txt\")\n",
    "test_token_seq, test_label_seq = read_conll2003(\"data/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOoNc1VUQELc"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HK8AcwWGQELd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu\tB-ORG\n",
      "rejects\tO\n",
      "german\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "british\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(train_token_seq[0], train_label_seq[0]):\n",
    "    print(f\"{token}\\t{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "K8SqDeMJjF3Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cricket\tO\n",
      "-\tO\n",
      "leicestershire\tB-ORG\n",
      "take\tO\n",
      "over\tO\n",
      "at\tO\n",
      "top\tO\n",
      "after\tO\n",
      "innings\tO\n",
      "victory\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(valid_token_seq[0], valid_label_seq[0]):\n",
    "    print(f\"{token}\\t{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ddFE7p5kjF_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer\tO\n",
      "-\tO\n",
      "japan\tB-LOC\n",
      "get\tO\n",
      "lucky\tO\n",
      "win\tO\n",
      ",\tO\n",
      "china\tB-PER\n",
      "in\tO\n",
      "surprise\tO\n",
      "defeat\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(test_token_seq[0], test_label_seq[0]):\n",
    "    print(f\"{token}\\t{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BZ4Go3IXfDit"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert len(train_token_seq) == len(train_label_seq), \"Длины тренировочных token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
    "assert len(valid_token_seq) == len(valid_label_seq), \"Длины валидационных token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
    "assert len(test_token_seq) == len(test_label_seq), \"Длины тестовых token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
    "\n",
    "assert train_token_seq[0] == ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'], \"Ошибка в тренировочном token_seq\"\n",
    "assert train_label_seq[0] == ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], \"Ошибка в тренировочном label_seq\"\n",
    "\n",
    "assert valid_token_seq[0] == ['cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top', 'after', 'innings', 'victory', '.'], \"Ошибка в валидационном token_seq\"\n",
    "assert valid_label_seq[0] == ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], \"Ошибка в валидационном label_seq\"\n",
    "\n",
    "assert test_token_seq[0] == ['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.'], \"Ошибка в тестовом token_seq\"\n",
    "assert test_label_seq[0] == ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O'], \"Ошибка в тестовом label_seq\"\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SVL4USbQELe"
   },
   "source": [
    "### Подготовка словарей\n",
    "\n",
    "Чтобы обучать нейронную сеть, мы будем использовать два отображения:\n",
    "- {**token**}→{**token_idx**}: соответствие между словом / токеном и индексом строки в *embedding* матрице (начинается с 0);\n",
    "- {**label**}→{**label_idx**}: соответствие между тегом и уникальным индексом (начинается с 0).\n",
    "\n",
    "Теперь нам необходимо реализовать две функции:\n",
    "- *get_token2idx*\n",
    "- *get_label2idx*\n",
    "\n",
    "которые будут возвращать соответствующие словари (*token2idx* и *label2idx*).\n",
    "\n",
    "Словарь *token2idx* должен содержать специальные токены, которые нужно добавить самим:\n",
    "- `<PAD>` - спецтокен для паддинга (отступа), так как мы собираемся обучать модели батчами. Токен `<PAD>` нужен для выравнивания предложений по длине, когда их будем помещать в один батч. Чаще всего предложения дополняются с конца;\n",
    "- `<UNK>` - спецтокен для обработки слов / токенов, которых нет в словаре (актуально для инференса).\n",
    "\n",
    "Давайте для удобства дадим токену `<PAD>` индекс `0`, а токену `<UNK>` индекс `1`.\n",
    "\n",
    "В функцию *get_token2idx* также необходимо добавить параметр *min_count*, который будет включать только слова, превышающие определенную частоту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSBooA__tPBP"
   },
   "source": [
    "Для начала посмотрим, а сколько вообще уникальных слов в обучающих данных и число их вхождений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IthnXKsoo7A3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 8390)\n",
      "('.', 7374)\n",
      "(',', 7290)\n",
      "('of', 3815)\n",
      "('in', 3621)\n",
      "('to', 3424)\n",
      "('a', 3199)\n",
      "('and', 2872)\n",
      "('(', 2861)\n",
      "(')', 2861)\n",
      "Количество уникальных слов в тренировочном датасете: 21010\n",
      "Количество слов встречающихся только один раз в тренировочном датасете: 10060\n"
     ]
    }
   ],
   "source": [
    "token_counter = Counter([token for sentence in train_token_seq for token in sentence])\n",
    "print(*token_counter.most_common(10), sep='\\n')\n",
    "print(f\"Количество уникальных слов в тренировочном датасете: {len(token_counter)}\")\n",
    "print(f\"Количество слов встречающихся только один раз в тренировочном датасете: {len([token for token, cnt in token_counter.items() if cnt == 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRtCHt1QruSU"
   },
   "source": [
    "Как мы видим, у нас есть много слов, которые в обучении встречаются только один раз. Очевидно, что выучиться по ним у нас не получиться, мы только переобучимся, поэтому давайте выкинем такие слова при формировании нашего словаря, задав параметр функции `min_count=2`.\n",
    "\n",
    "На этом этапе можно применять различные методы сокращения размера словаря, преобразовывая разные словоформы одного слова в один токен: стемминг или лемматизация. В текущей задаче мы это опустим, но в некоторых ситуациях это бывает полезно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOnc3UHpQELf"
   },
   "source": [
    "**Задание. Реализуйте функции get_token2idx и get_label2idx.** **<font color='red'>(1 балл)</font>**\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №1 </summary>\n",
    "\n",
    "*Не забудьте, что у \\<PAD\\> индекс 0, а у \\<UNK\\> индекс 1.*\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №2 </summary>\n",
    "\n",
    "*Лучше всего словари token2idx и label2idx собирать с помощью token2cnt в get_token2idx и label_list в get_label2idx соответственно.*\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №3 </summary>\n",
    "\n",
    "*В label_list в get_label2idx лежат отсортированные по возрастанию тэги, за исключением тэга 'O' – он идет первый (индекс 0). Этот порядок нужно сохранить при индексации в label2idx.*\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "aCaPftCyQELi"
   },
   "outputs": [],
   "source": [
    "def get_token2idx(\n",
    "    token_seq: List[List[str]],\n",
    "    min_count: int,\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get mapping from tokens to indices to use with Embedding layer.\n",
    "\n",
    "    Args:\n",
    "        token_seq: The list of lists. Each internal list (sentence)\n",
    "            consists of tokens.\n",
    "        min_count:  The minimum number of repetitions of\n",
    "            a token in the corpus.\n",
    "\n",
    "    Returns:\n",
    "        Function returns mapping from token to id.\n",
    "        token2idx: The mapping from token\n",
    "            to id without \"rare\" words.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    token2idx: Dict[str, int] = {'<PAD>': 0, '<UNK>': 1}\n",
    "    token2cnt = Counter([token for sentence in token_seq for token in sentence])\n",
    "\n",
    "    idx = 2\n",
    "    for token, count in token2cnt.items():\n",
    "        if count >= min_count:\n",
    "            token2idx[token] = idx\n",
    "            idx += 1\n",
    "\n",
    "    return token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uFK130y-sLH4"
   },
   "outputs": [],
   "source": [
    "token2idx = get_token2idx(train_token_seq, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "t6i51GPtQELj"
   },
   "outputs": [],
   "source": [
    "def get_label2idx(label_seq: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get mapping from labels to indices.\n",
    "\n",
    "    Args:\n",
    "        label_seq: The list of lists. Each internal list (sentence)\n",
    "            consists of labels.\n",
    "\n",
    "    Returns:\n",
    "        Function returns mapping from label to id.\n",
    "        label2idx: The mapping from label to id.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    label2idx: Dict[str, int] = {}\n",
    "    label_list = set(label for sentence in label_seq for label in sentence)\n",
    "    label_list = sorted(label_list, key=lambda x: 'A' if x == 'O' else x)\n",
    "\n",
    "    for idx, label in enumerate(label_list):\n",
    "        label2idx[label] = idx\n",
    "\n",
    "    return label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XW6fK0HtQELk"
   },
   "outputs": [],
   "source": [
    "label2idx = get_label2idx(train_label_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U13l-2IOQELk"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "O7U7bMrHQELl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\t0\n",
      "<UNK>\t1\n",
      "eu\t2\n",
      "german\t3\n",
      "call\t4\n",
      "to\t5\n",
      "boycott\t6\n",
      "british\t7\n",
      "lamb\t8\n",
      ".\t9\n"
     ]
    }
   ],
   "source": [
    "for token, idx in list(token2idx.items())[:10]:\n",
    "    print(f\"{token}\\t{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Hp75V-o2QELl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\t0\n",
      "B-LOC\t1\n",
      "B-MISC\t2\n",
      "B-ORG\t3\n",
      "B-PER\t4\n",
      "I-LOC\t5\n",
      "I-MISC\t6\n",
      "I-ORG\t7\n",
      "I-PER\t8\n"
     ]
    }
   ],
   "source": [
    "for label, idx in label2idx.items():\n",
    "    print(f\"{label}\\t{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VYb4BdAUhNzk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_token2idx(train_token_seq, min_count=1)) == 21012, \"Ошибка в длине словаря, скорее всего неверно реализован min_count\"\n",
    "assert len(token2idx) == 10952, \"Неправильная длина token2idx, скорее всего неверно реализован min_count\"\n",
    "assert len(label2idx) == 9, \"Неправильная длина label2idx\"\n",
    "\n",
    "assert list(token2idx.items())[:10] == [('<PAD>', 0), ('<UNK>', 1), ('eu', 2), ('german', 3), ('call', 4), ('to', 5), ('boycott', 6), ('british', 7), ('lamb', 8), ('.', 9)], \"Неправильно сформированный token2idx\"\n",
    "assert label2idx == {'O': 0, 'B-LOC': 1, 'B-MISC': 2, 'B-ORG': 3, 'B-PER': 4, 'I-LOC': 5, 'I-MISC': 6, 'I-ORG': 7, 'I-PER': 8}, \"Неправильно сформированный label2idx\"\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItPs1DmOQELm"
   },
   "source": [
    "### Подготовка датасета и загрузчика\n",
    "\n",
    "Обычно нейронные сети обучаются батчами. Это означает, что каждое обновление весов нейронной сети происходит на основе нескольких последовательностей. Технической деталью является необходимость дополнить все последовательности внутри батча до одной длины.\n",
    "\n",
    "Из предыдущего практического задания вы должны знать о `Dataset`'е (`torch.utils.data.Dataset`) – структура данных, которая хранит и может по индексу отдавать данные для обучения. Датасет должен наследоваться от стандартного PyTorch класса Dataset и переопределять методы `__len__` и `__getitem__`.\n",
    "\n",
    "Метод `__getitem__` должен возвращать индексированную последовательность и её теги.\n",
    "\n",
    "**Не забудьте** про `<UNK>` спецтокен для неизвестных слов!\n",
    "    \n",
    "Давайте напишем кастомный датасет под нашу задачу, который на вход (метод `__init__`) будет принимать:\n",
    "- *token_seq* – список списков слов / токенов;\n",
    "- *label_seq* – список списков тегов;\n",
    "- *token2idx* – отображение токена в индекс;\n",
    "- *label2idx* – отображение тега в индекс.\n",
    "\n",
    "и возвращать из метода `__getitem__` два Int64 тензора (`torch.LongTensor`) из индексов слов / токенов в сэмпле и индексов соответвующих тегов:\n",
    "\n",
    "**Задание. Реализуйте класс датасета NERDataset.** **<font color='red'>(1 балл)</font>**\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка </summary>\n",
    "\n",
    "*Для понимания, зачем нужен вообще этот Dataset. У нас по факту есть \"сырые\" последовательности token_seq и label_seq, которые по индексу будут возвращать и нужный набор токенов, и соответствующий список тегов. Но для обучения это неудобно, потому что каждый раз нам необходимо конвертировать токены и теги в индексы. Инструмент Dataset нужен для того, чтобы, не задумываясь, извлекать элементы в нужном формате автоматически, как из массива.*\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kdZvnUUpQELm"
   },
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for NER.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_seq: List[List[str]],\n",
    "        label_seq: List[List[str]],\n",
    "        token2idx: Dict[str, int],\n",
    "        label2idx: Dict[str, int],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor of NERDataset class.\n",
    "\n",
    "        Args:\n",
    "            token_seq: The list of lists. Each internal list (sentence)\n",
    "                consists of tokens.\n",
    "            label_seq: The list of lists. Each internal list (sentence)\n",
    "                consists of labels.\n",
    "            token2idx: The mapping from token to id.\n",
    "            label2idx: The mapping from label to id.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        self.token2idx = token2idx\n",
    "        self.label2idx = label2idx\n",
    "\n",
    "        self.token_seq = [self.process_tokens(tokens, token2idx) for tokens in token_seq]\n",
    "        self.label_seq = [self.process_labels(labels, label2idx) for labels in label_seq]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get dataset size.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "            The method returns the length of the dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        return len(self.token_seq)\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        Get an element from a dataset by index.\n",
    "        Recomendation: use LongTensor.\n",
    "\n",
    "        Args:\n",
    "            idx: Index of element (int).\n",
    "\n",
    "        Returns:\n",
    "            The method returns required element. From\n",
    "            self.token_seq and self.label_seq.\n",
    "\n",
    "        \"\"\"\n",
    "        token_ids = torch.LongTensor(self.token_seq[idx])\n",
    "        label_ids = torch.LongTensor(self.label_seq[idx])\n",
    "        \n",
    "        return token_ids, label_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def process_tokens(\n",
    "        tokens: List[str],\n",
    "        token2idx: Dict[str, int],\n",
    "        unk: str = \"<UNK>\",\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Transform list of tokens into list of tokens' indices.\n",
    "\n",
    "        Args:\n",
    "            tokens: The list (sentence) of tokens.\n",
    "            token2idx: The mapping from token to id.\n",
    "            unk: Name of <UNK> token (str).\n",
    "\n",
    "        Returns:\n",
    "            token_ids: The list of indices. Each index\n",
    "                corresponds to a token from the tokens.\n",
    "\n",
    "        \"\"\"\n",
    "        token_ids: List[int] = []\n",
    "\n",
    "        for token in tokens:\n",
    "            token_ids.append(token2idx.get(token, token2idx[unk]))\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def process_labels(\n",
    "        labels: List[str],\n",
    "        label2idx: Dict[str, int],\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Transform list of labels into list of labels' indices.\n",
    "\n",
    "        Args:\n",
    "            labels: The list of labels.\n",
    "            label2idx: The mapping from label to id.\n",
    "\n",
    "        Returns:\n",
    "            label_ids: The list of indices. Each index\n",
    "                corresponds to a label from the labels.\n",
    "\n",
    "        \"\"\"\n",
    "        label_ids: List[int] = []\n",
    "\n",
    "        for label in labels:\n",
    "            label_ids.append(label2idx[label])\n",
    "\n",
    "        return label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCvaPJERQELn"
   },
   "source": [
    "Создадим три датасета:\n",
    "- *train_dataset*\n",
    "- *valid_dataset*\n",
    "- *test_dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bUMsSNkoQELn"
   },
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(\n",
    "    token_seq=train_token_seq,\n",
    "    label_seq=train_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "valid_dataset = NERDataset(\n",
    "    token_seq=valid_token_seq,\n",
    "    label_seq=valid_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "test_dataset = NERDataset(\n",
    "    token_seq=test_token_seq,\n",
    "    label_seq=test_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQIq1pAWQELo"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wDlhYu-YW0s3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 1, 3, 4, 5, 6, 7, 8, 9]), tensor([3, 0, 2, 0, 0, 0, 2, 0, 0]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CCURUIeKW3DG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1737,  571, 1777,  197,  687,  145,  349,  111, 1819, 1558,    9]),\n",
       " tensor([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zyAazaLzjQ-K"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1516,  571, 1434, 1729, 4893, 2014,   67,  310,  215, 3157, 3139,    9]),\n",
       " tensor([0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Gox6uyF2idwZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert len(train_dataset) == 14986, \"Неправильная длина train_dataset\"\n",
    "assert len(valid_dataset) == 3465, \"Неправильная длина valid_dataset\"\n",
    "assert len(test_dataset) == 3683, \"Неправильная длина test_dataset\"\n",
    "\n",
    "assert torch.equal(train_dataset[0][0], torch.tensor([2,1,3,4,5,6,7,8,9])), \"Неправильно сформированный train_dataset\"\n",
    "assert torch.equal(train_dataset[0][1], torch.tensor([3,0,2,0,0,0,2,0,0])), \"Неправильно сформированный train_dataset\"\n",
    "\n",
    "assert torch.equal(valid_dataset[0][0], torch.tensor([1737,571,1777,197,687,145,349,111,1819,1558,9])), \"Неправильно сформированный valid_dataset\"\n",
    "assert torch.equal(valid_dataset[0][1], torch.tensor([0,0,3,0,0,0,0,0,0,0,0])), \"Неправильно сформированный valid_dataset\"\n",
    "\n",
    "assert torch.equal(test_dataset[0][0], torch.tensor([1516,571,1434,1729,4893,2014,67,310,215,3157,3139,9])), \"Неправильно сформированный test_dataset\"\n",
    "assert torch.equal(test_dataset[0][1], torch.tensor([0,0,1,0,0,0,0,4,0,0,0,0])), \"Неправильно сформированный test_dataset\"\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWjJuAk7QELp"
   },
   "source": [
    "Для того, чтобы дополнять последовательности паддингом, будем использовать параметр `collate_fn` класса `DataLoader`.\n",
    "\n",
    "Принимая последовательность пар тензоров для предложений и тегов, необходимо дополнить все последовательности до последовательности максимальной длины в батче.\n",
    "\n",
    "Используйте для дополнения спецтокен `<PAD>` для последовательностей слов / токенов и -1 для последовательностей тегов.\n",
    "\n",
    "**hint**: удобно использовать метод **torch.nn.utils.rnn**. Обратите особое внимание на параметр *batch_first*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZiJVM5qQELp"
   },
   "source": [
    "`Collator` можно реализовать двумя способами:\n",
    "- класс с методом `__call__`\n",
    "- функцию\n",
    "\n",
    "Мы пойдем первым путем.\n",
    "\n",
    "Инициализировать экземпляр класса `Collator` (метод `__init__`) с помощью двух параметров:\n",
    "- id `<PAD>` спецтокена для последовательностей слов / токенов\n",
    "- id `<PAD>` спецтокена для последовательностей тегов (значение -1)\n",
    "\n",
    "Метод `__call__` на вход принимает батч, а именно список кортежей того, что нам возвращается из датасета. В нашем случае это список кортежей двух int64 тензоров - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
    "\n",
    "На выходе мы хотим получить два тензора:\n",
    "- западденные индексы слов / токенов\n",
    "- западденные индексы тегов\n",
    "    \n",
    "P.S. `<PAD>` значение нужно для того, чтобы при подсчете лосса легко отличать западдированные токены от других. Можно использовать параметр *ignore_index* при инициализации лосса.\n",
    "\n",
    "**Задание. Реализуйте класс коллатора NERCollator.** **<font color='red'>(1 балл)</font>**\n",
    "\n",
    "<details>\n",
    "<summary> Пояснение </summary>\n",
    "\n",
    "*Чтобы ускорить взаимодействие с нейронными сетями, им на вход подаются не одна пара (объект, ответ), а несколько. Они объединяются в одну связку, которую называют батч. В нашем случае объекты -- это последовательности слов, которые имеют разную длину. Мы не можем в одном pytorch.tensor (или numpy.array) хранить последовательности разных длин, поэтому требуется их привести к одной (например, к длине максимальной последовательности), чем коллатор и занимается.*\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "LNHNwoLnQELp"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class NERCollator:\n",
    "    \"\"\"\n",
    "    Collator that handles variable-size sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_padding_value: int,\n",
    "        label_padding_value: int,\n",
    "    ):\n",
    "        self.token_padding_value = token_padding_value\n",
    "        self.label_padding_value = label_padding_value\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: List[Tuple[torch.LongTensor, torch.LongTensor]],\n",
    "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        The method appends <PAD> token id to tensor with\n",
    "        token ids and -1 to tensor with labels.\n",
    "        The function torch.nn.utils.rnn.pad_sequence is useful for padding. Link:\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n",
    "\n",
    "        Args:\n",
    "            batch: The list of tuples. Each tuple conists of the tensor with token ids and\n",
    "                the tensor with label ids.\n",
    "\n",
    "        Returns:\n",
    "            (tokens, labels): a pair of tensors with sizes: (batch_size, sentence_len).\n",
    "\n",
    "        \"\"\"\n",
    "        tokens, labels = zip(*batch)\n",
    "\n",
    "        tokens = pad_sequence(tokens, batch_first=True, padding_value=self.token_padding_value)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=self.label_padding_value)\n",
    "\n",
    "        return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nZUMwVQTQELq"
   },
   "outputs": [],
   "source": [
    "collator = NERCollator(\n",
    "    token_padding_value=token2idx[\"<PAD>\"],\n",
    "    label_padding_value=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jsgfij8WQELq"
   },
   "source": [
    "Теперь всё готово, чтобы задать `DataLoader`'ы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "gFljkiBOQELr"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i34wGJ4uQELr"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "QLlr_DztQELr"
   },
   "outputs": [],
   "source": [
    "tokens, labels = next(iter(train_dataloader))\n",
    "\n",
    "tokens = tokens.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zQvYRqEFtmpz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7796, 1162, 2553, 7237, 1342,    0,    0,    0,    0,    0],\n",
       "        [ 125, 1167,    1,   67, 1349,  489, 1215, 1364, 1365, 1366]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "w--fhADKQELs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  0,  3,  7,  0, -1, -1, -1, -1, -1],\n",
       "        [ 0,  4,  8,  0,  1,  0,  0,  0,  0,  0]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "yFeX0AYKlhGk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(train_tokens, torch.tensor([[ 2,  1,  3,  4,  5,  6,  7,  8,  9], [10, 11,  0,  0,  0,  0,  0,  0,  0]])), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(train_labels, torch.tensor([[ 3,  0,  2,  0,  0,  0,  2,  0,  0], [ 4,  8, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "valid_tokens, valid_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(valid_tokens, torch.tensor([[ 1737,   571,  1777,   197,   687,   145,   349,   111,  1819,  1558, 9], [  248, 10679,     0,     0,     0,     0,     0,     0,     0,     0,    0]])), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(valid_labels, torch.tensor([[ 0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0], [ 1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "test_tokens, test_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(test_tokens, torch.tensor([[1516,  571, 1434, 1729, 4893, 2014,   67,  310,  215, 3157, 3139,    9], [   1,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(test_labels, torch.tensor([[ 0,  0,  1,  0,  0,  0,  0,  4,  0,  0,  0,  0], [ 4,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ul5gLriQELs"
   },
   "source": [
    "## Часть 2. BiLSTM-теггер (6 баллов)\n",
    "\n",
    "Определите архитектуру сети, используя библиотеку PyTorch.\n",
    "\n",
    "Ваша архитектура в этом пункте должна соответствовать стандартному теггеру:\n",
    "* Embedding слой на входе\n",
    "* LSTM (однонаправленный или двунаправленный) слой для обработки последовательности\n",
    "* Dropout (заданный отдельно или встроенный в LSTM) для уменьшения переобучения\n",
    "* Linear слой на выходе\n",
    "\n",
    "Для обучения сети используйте поэлементную кросс-энтропийную функцию потерь.\n",
    "\n",
    "**Обратите внимание**, что `<PAD>` токены не должны учавствовать в подсчёте функции потерь. В качестве оптимизатора рекомендуется использовать Adam. Для получения значений предсказаний по выходам модели используйте функцию `argmax`.\n",
    "\n",
    "**Задание. Реализуйте класс модели BiLSTM.** **<font color='red'>(2 балл)</font>**\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №1 </summary>\n",
    "\n",
    "*Помните, что следуется явно указывать параметр `batch_first=True`.*\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №2 </summary>\n",
    "\n",
    "*Число входных признаков `in_features` в линейном слое зависим от типа LSTM: однонаправленная или двунаправленая. В первом случае выходом i-го блока является скрытое состояние $h_i$, которое имеет размер `hidden_size`, и соответственно, `in_features=hidden_size`. А в случае двунаправленной сети вход представим в виде конкатенации скрытых состояний из разных уровней сети: $[h_i^1, h_i^2]$, здесь `in_features=2 * hidden_size`.*\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> Рекомендация </summary>\n",
    "\n",
    "*Обратите внимание на метод `BiLSTM.forward`, реализовывать самостоятельно его не нужно. В нем используется функция `pack_padded_sequence` и обратная ей `pad_packed_sequence`. Это удобный инструмент для фильтрации входных последовательностей от токенов отступа.*\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "uMiLQljZQELt"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Embedding, LSTM, Linear\n",
    "\n",
    "\n",
    "class BiLSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        bidirectional: bool,\n",
    "        n_classes: int,\n",
    "        token_padding_value: int,\n",
    "        max_norm: float,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        BiLSTM class constructor. The method contains a description\n",
    "        of the network layers and their parameters.\n",
    "\n",
    "        Args:\n",
    "            num_embeddings: The number of unique words (tokens) in the\n",
    "                dictionary, or the size of the dictionary (int)\n",
    "            embedding_dim:  Embedding word size (size)\n",
    "            hidden_size: The size of the hidden state (h_n) in the LSTM network (int)\n",
    "            num_layers: The total number of blocks in an LSTM network (int)\n",
    "            dropout: dropout parameter in LSTM layers (float)\n",
    "            bidirectional: a parameter that controls whether the LSTM is\n",
    "                unidirectional (one direction) or bidirectional (bool)\n",
    "            n_classes: the number of classes in a problem being solved (int)\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Tips:\n",
    "            - do not forget to specify batch_first=True\n",
    "            - control the size of the in_features in linear\n",
    "                layer depending on the value of 'bidirectional'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_padding_value = token_padding_value\n",
    "        self.embedding = None # Embedding layer\n",
    "        self.rnn = None # LSTM layer\n",
    "        self.head = None # Linear layer\n",
    "\n",
    "        self.embedding = Embedding(num_embeddings, embedding_dim, padding_idx=token_padding_value)\n",
    "\n",
    "        self.rnn = LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        in_features = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.head = Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, tokens: torch.LongTensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applying neural network layers to input 'tokens'.\n",
    "\n",
    "        Args:\n",
    "            tokens: the input tensor with tokens ids (batch_size, sequence_len)\n",
    "\n",
    "        Returns:\n",
    "            logits: the scores issued by the model (batch_size, num_classes, sequence_len)\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens)\n",
    "\n",
    "        # Используем специальную функцию pack_padded_sequence для того, чтобы получить\n",
    "        # структуру PackedSequence, которая не учитывать паддинг при проходе rnn.\n",
    "        # lengths -- длины исходных исходных последовательностей в батче,\n",
    "        # без учёта сдвига\n",
    "        lengths = (tokens != self.token_padding_value).sum(dim=1).detach().cpu()\n",
    "        packed_embed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            input=embed,\n",
    "            lengths=lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # Используем специальную функцию pad_packed_sequence для того, чтобы получить\n",
    "        # тензор из PackedSequence. Операция является обратной к pack_padded_sequence\n",
    "        packed_rnn_output, _ = self.rnn(packed_embed)\n",
    "        rnn_output, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=packed_rnn_output,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        logits = self.head(rnn_output)\n",
    "        logits = logits.transpose(1, 2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "zps8HL2VQELu"
   },
   "outputs": [],
   "source": [
    "model = BiLSTM(\n",
    "    num_embeddings=len(token2idx),\n",
    "    embedding_dim=300,\n",
    "    hidden_size=256,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    bidirectional=True,\n",
    "    n_classes=len(label2idx),\n",
    "    token_padding_value=token2idx[\"<PAD>\"],\n",
    "    max_norm=None,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "nmg2_C_oQELu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(10952, 300, padding_idx=0)\n",
       "  (rnn): LSTM(300, 256, batch_first=True, bidirectional=True)\n",
       "  (head): Linear(in_features=512, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "sDvWB5J2QELv"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Jn5Pu1UKQELv"
   },
   "outputs": [],
   "source": [
    "outputs = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "n02Bsh8eQELw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert outputs.shape == torch.Size([2, 9, 10])\n",
    "assert 2 < criterion(outputs, labels) < 3\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjhkK9QFQELu"
   },
   "source": [
    "### Эксперименты\n",
    "\n",
    "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1-macro мере на валидационной и тестовой выборках было не меньше **0.76**.\n",
    "\n",
    "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "f4hdrFZ9iRPi"
   },
   "outputs": [],
   "source": [
    "# Создадим SummaryWriter для эксперимента с BiLSTMModel\n",
    "# для отслеживания процесса обучения нейронной сети\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"logs/BiLSTMModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruMTBSkQQELx"
   },
   "source": [
    "**Задание. Реализуйте функцию подсчета метрик compute_metrics.** **<font color='red'>(1 балл)</font>**\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №1 </summary>\n",
    "\n",
    "*Модель выдает логиты, или скоры, для каждого токена по каждому классу. Для подсчета метрик нужно, подобно максимизации вероятности, каждому токену входной последовательности определять класс с максимальный скором. Пример: токен `X` получил скоры (выход модели) для четырех классов `[0.5, 10.2, -13,9, 5,5]` соответственно, следовательно, нам необходимо определять для токена `X` класс № 1 (нумерация с нуля, `score=10.2`) как наиболее \"вероятный\".*\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №2 </summary>\n",
    "\n",
    "*Входные тензоры необходимо перевести на CPU (если они на GPU), конвертировать в numpy-массив и для простоты вытянуть в вектор с помощью функции `flatten`.*\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №3 </summary>\n",
    "\n",
    "*Не забудьте отфильтровать `<PAD>` токен.*\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "xkpo3JgWQELx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    outputs: torch.Tensor,\n",
    "    labels: torch.LongTensor,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute NER metrics.\n",
    "\n",
    "    Args:\n",
    "        outputs: the model outputs (batch_size, num_classes, sequence_len)\n",
    "        labels: the correct classes (batch_size, sequence_len)\n",
    "\n",
    "    Returns:\n",
    "        metrics: mapping metric names to their corresponding values\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {}\n",
    "    y_true = None\n",
    "    y_pred = None\n",
    "\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    y_pred = outputs.argmax(axis=1)\n",
    "    valid_mask = labels != -1\n",
    "    y_true = labels[valid_mask]\n",
    "    y_pred = y_pred[valid_mask]\n",
    "    \n",
    "    metrics['accuracy'] = accuracy_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "    )\n",
    "\n",
    "    for metric_func in [precision_score, recall_score, f1_score]:\n",
    "        metric_name = metric_func.__name__.split('_')[0]\n",
    "        for average_type in [\"micro\", \"macro\", \"weighted\"]:\n",
    "            metrics[metric_name + '_' + average_type] = metric_func(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                average=average_type,\n",
    "                zero_division=0,\n",
    "            )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "1mYqYD-LNiNh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.06666666666666667,\n",
       " 'precision_micro': 0.06666666666666667,\n",
       " 'precision_macro': 0.0625,\n",
       " 'precision_weighted': 0.3,\n",
       " 'recall_micro': 0.06666666666666667,\n",
       " 'recall_macro': 0.013888888888888888,\n",
       " 'recall_weighted': 0.06666666666666667,\n",
       " 'f1_micro': 0.06666666666666667,\n",
       " 'f1_macro': 0.022727272727272728,\n",
       " 'f1_weighted': 0.1090909090909091}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(\n",
    "    outputs=outputs,\n",
    "    labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzj89UygQEL0"
   },
   "source": [
    "**Задание. Реализуйте функции обучения и тестирования train_epoch и evaluate_epoch.** **<font color='red'>(2 балла)</font>**\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №1 </summary>\n",
    "\n",
    "*Почти всегда шаг обучения модели остается неизменным. Нужно выполнить последовательность действий: обнулить градиент, получить выходы модели, посчитать лосс, посчитать новые градиенты через `.backward()`, сделать шаг оптимизатора.*\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №2 </summary>\n",
    "\n",
    "*При evaluate-этапе никакие градиенты не вычисляются, потому нет необходимости ни их в обнулении, ни в оптимизации по ним.*\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> Пояснение </summary>\n",
    "\n",
    "*В PyTorch градиенты аккумулируются, их нужно сбрасывать через `optimizer.zero_grad()` перед каждой новой итерацией, чтобы исключить влияние градиента с предыдущих итераций на шаг текущего.*\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "sG3vQbc_QEL0"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    model_type: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One training cycle (loop).\n",
    "\n",
    "    Args:\n",
    "        model: BiLSTM model\n",
    "        dataloader: Dataloader with train data\n",
    "        optimizer: an algorithm for model optimization\n",
    "        criterion: the loss function\n",
    "        writer: a tool for logging the learning process\n",
    "        device: the device on which the model will work\n",
    "        epoch: the total number of epochs\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    for i, (tokens, labels) in tqdm(\n",
    "        enumerate(dataloader),\n",
    "        total=len(dataloader),\n",
    "        desc=\"loop over train batches\",\n",
    "    ):\n",
    "\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "        outputs = None\n",
    "        loss = None\n",
    "\n",
    "        if model_type == 'BiLSTM':\n",
    "            logits = model.forward(tokens)\n",
    "            outputs = logits\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        elif model_type == 'Transformer':\n",
    "            logits = model.forward(**tokens).logits\n",
    "            loss = criterion(logits.transpose(1, 2), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            raise ValueError('Use \\'BiLSTM\\' or \\'Transformer\\' model_type.')\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        writer.add_scalar(\n",
    "            tag=\"batch loss / train\",\n",
    "            scalar_value=loss.item(),\n",
    "            global_step=epoch * len(dataloader) + i,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            if model_type == 'BiLSTM':\n",
    "                outputs_inference = model(tokens)\n",
    "            elif model_type == 'Transformer':\n",
    "                outputs_inference = model(**tokens)[\"logits\"].transpose(1, 2)\n",
    "            else:\n",
    "                raise ValueError('Use \\'BiLSTM\\' or \\'Transformer\\' model_type.')\n",
    "            model.train()\n",
    "\n",
    "        batch_metrics = compute_metrics(\n",
    "            outputs=outputs_inference,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        for metric_name, metric_value in batch_metrics.items():\n",
    "            batch_metrics_list[metric_name].append(metric_value)\n",
    "            writer.add_scalar(\n",
    "                tag=f\"batch {metric_name} / train\",\n",
    "                scalar_value=metric_value,\n",
    "                global_step=epoch * len(dataloader) + i,\n",
    "            )\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f\"Train loss: {avg_loss}\\n\")\n",
    "    writer.add_scalar(\n",
    "        tag=\"loss / train\",\n",
    "        scalar_value=avg_loss,\n",
    "        global_step=epoch,\n",
    "    )\n",
    "\n",
    "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "        metric_value = np.mean(metric_value_list)\n",
    "        print(f\"Train {metric_name}: {metric_value}\\n\")\n",
    "        writer.add_scalar(\n",
    "            tag=f\"{metric_name} / train\",\n",
    "            scalar_value=metric_value,\n",
    "            global_step=epoch,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "p4ztFogtQEL0"
   },
   "outputs": [],
   "source": [
    "def evaluate_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    model_type: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One evaluation cycle (loop).\n",
    "\n",
    "    Args:\n",
    "        model: BiLSTM model\n",
    "        dataloader: Dataloader with data for evaluation\n",
    "        criterion: a loss function\n",
    "        writer: a tool for logging the learning process\n",
    "        device: the device on which the model will work\n",
    "        epoch: the total number of epochs\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (tokens, labels) in tqdm(\n",
    "            enumerate(dataloader),\n",
    "            total=len(dataloader),\n",
    "            desc=\"loop over test batches\",\n",
    "        ):\n",
    "\n",
    "            tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "            if model_type == 'BiLSTM':\n",
    "                logits = model.forward(tokens)\n",
    "                outputs = logits\n",
    "                loss = criterion(logits, labels)\n",
    "            elif model_type == 'Transformer':\n",
    "                logits = model.forward(**tokens).logits\n",
    "                outputs = logits.transpose(1, 2)\n",
    "                loss = criterion(logits.transpose(1, 2), labels)\n",
    "            else:\n",
    "                raise ValueError('Use \\'BiLSTM\\' or \\'Transformer\\' model_type.')\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            writer.add_scalar(\n",
    "                tag=\"batch loss / test\",\n",
    "                scalar_value=loss.item(),\n",
    "                global_step=epoch * len(dataloader) + i,\n",
    "            )\n",
    "\n",
    "            batch_metrics = compute_metrics(\n",
    "                outputs=outputs,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            for metric_name, metric_value in batch_metrics.items():\n",
    "                batch_metrics_list[metric_name].append(metric_value)\n",
    "                writer.add_scalar(\n",
    "                    tag=f\"batch {metric_name} / test\",\n",
    "                    scalar_value=metric_value,\n",
    "                    global_step=epoch * len(dataloader) + i,\n",
    "                )\n",
    "\n",
    "        avg_loss = np.mean(epoch_loss)\n",
    "        print(f\"Test loss:  {avg_loss}\\n\")\n",
    "        writer.add_scalar(\n",
    "            tag=\"loss / test\",\n",
    "            scalar_value=avg_loss,\n",
    "            global_step=epoch,\n",
    "        )\n",
    "\n",
    "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "            metric_value = np.mean(metric_value_list)\n",
    "            print(f\"Test {metric_name}: {metric_value}\\n\")\n",
    "            writer.add_scalar(\n",
    "                tag=f\"{metric_name} / test\",\n",
    "                scalar_value=np.mean(metric_value),\n",
    "                global_step=epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "z7Z5MTNzQEL1"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    n_epochs: int,\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    valid_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    model_type: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Training loop.\n",
    "\n",
    "    Args:\n",
    "        n_epochs: the total number of epochs in training\n",
    "        model: BiLSTM model\n",
    "        train_dataloader:  Dataloader with train data\n",
    "        valid_dataloader: Dataloader with data for evaluation\n",
    "        optimizer: an algorithm for model optimization\n",
    "        criterion: a loss function\n",
    "        writer: a tool for logging the learning process\n",
    "        device: the device on which the model will work\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
    "\n",
    "        train_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            writer=writer,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "            model_type=model_type,\n",
    "        )\n",
    "        evaluate_epoch(\n",
    "            model=model,\n",
    "            dataloader=valid_dataloader,\n",
    "            criterion=criterion,\n",
    "            writer=writer,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "            model_type=model_type,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTxfU0BfQEL1"
   },
   "source": [
    "**Задание. Проведите эксперименты.** **<font color='red'>(2 балла)</font>**\n",
    "\n",
    "Настало время собрать все воедино. В этом блоке предлагается подбирать разные параметры, чтобы достичь качества F1-macro на тестовой и валидационной выборках не менее **0.76**.\n",
    "\n",
    "Будем задействовать ранее определенные `test_dataloader`, `valid_dataloader`, `criretion`. Настраивайте параметры на валидационной выборке так, чтобы получить требуемое качество. Основные из них:\n",
    "- `n_epoch` -- число эпох обучения, рекомендуется выбирать от 5 до 20;\n",
    "- `embedding_dim` -- размерность эмбеддингов, рекомендуем выбирать от 8 до 526;\n",
    "- `hidden_size` -- размерность скрытого состояния, от 8 до 1024;\n",
    "- `batch_size` -- размер обучающий батчей, от 8 до 128;\n",
    "- `dropout` -- параметр регуляризатора dropout, от 0 до 0.7;\n",
    "- `max_norm` -- максимальное органичение на норму эмбеддингов, 1.0 или `None`;\n",
    "- `lr` -- шаг оптимизатора, от 1e-3 до 1e-7.\n",
    "\n",
    "На практике используют еще более широкий набор регулировок, которые в положительную сторону влияют на качество. В текущем задании они не используются, но стоит знать, что:\n",
    "\n",
    "- в процессе обучения используют `gradient clipping`, чтобы контролировать норму градиентов. Величина должна быть согласована с `max_norm`, если такая используется;\n",
    "- изменение `lr` в процессе обучения, например, уменьшение с каждой эпохой. В трансформерах это отдельная проблема, которая при неправильном выборе `lr` приводит к серьезному переобучению. Типичной практикой является использование планировщиков `lr`: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate;\n",
    "- иногда выбор другого оптимизатора позволяет поднять качество: https://pytorch.org/docs/stable/optim.html#algorithms;\n",
    "- встаивают дополнительные регуляризационные блоки и регуляризационные механизмы, например, L2-норму.\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №1 </summary>\n",
    "\n",
    "*Следите за лоссом и метриками. Если в течение первых пяти эпохах нет роста качества, то скорее всего что-то не так.*\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> Подсказка №2 </summary>\n",
    "\n",
    "*Подсказка 2: попробуйте начать с параметров `embedding_dim=300` и `hidden_size=256`, `dropout=0.0`*\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "3gfxTDI3M3jQ"
   },
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "lr = 1e-4\n",
    "max_norm = 1.0\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "model = BiLSTM(\n",
    "    num_embeddings=len(token2idx),\n",
    "    embedding_dim=300,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.0,\n",
    "    bidirectional=True,\n",
    "    n_classes=len(label2idx),\n",
    "    token_padding_value=token2idx[\"<PAD>\"],\n",
    "    max_norm=None,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98))\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yz6mjGZUQEL2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55031015f69f4064a2d190cf9ee7b3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9990620514179798\n",
      "\n",
      "Train accuracy: 0.8283567565499559\n",
      "\n",
      "Train precision_micro: 0.8283567565499559\n",
      "\n",
      "Train precision_macro: 0.12397150557692323\n",
      "\n",
      "Train precision_weighted: 0.7043936214871442\n",
      "\n",
      "Train recall_micro: 0.8283567565499559\n",
      "\n",
      "Train recall_macro: 0.11687934304548514\n",
      "\n",
      "Train recall_weighted: 0.8283567565499559\n",
      "\n",
      "Train f1_micro: 0.8283567565499559\n",
      "\n",
      "Train f1_macro: 0.10938991799068805\n",
      "\n",
      "Train f1_weighted: 0.757339796239839\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88818c76aa7c48ccb46d9bcc963abce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.8310233910800553\n",
      "\n",
      "Test accuracy: 0.7859147086359789\n",
      "\n",
      "Test precision_micro: 0.7859147086359789\n",
      "\n",
      "Test precision_macro: 0.4481783332005\n",
      "\n",
      "Test precision_weighted: 0.6779019930794971\n",
      "\n",
      "Test recall_micro: 0.7859147086359789\n",
      "\n",
      "Test recall_macro: 0.5071474840851854\n",
      "\n",
      "Test recall_weighted: 0.7859147086359789\n",
      "\n",
      "Test f1_micro: 0.7859147086359789\n",
      "\n",
      "Test f1_macro: 0.46972581053868284\n",
      "\n",
      "Test f1_weighted: 0.7197948154288585\n",
      "\n",
      "Epoch [2 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8142beca4bb4bc6a91e74abd7886819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6833223952891979\n",
      "\n",
      "Train accuracy: 0.8364565502950163\n",
      "\n",
      "Train precision_micro: 0.8364565502950163\n",
      "\n",
      "Train precision_macro: 0.23905802270297327\n",
      "\n",
      "Train precision_weighted: 0.7438666136494341\n",
      "\n",
      "Train recall_micro: 0.8364565502950163\n",
      "\n",
      "Train recall_macro: 0.14082103521405037\n",
      "\n",
      "Train recall_weighted: 0.8364565502950163\n",
      "\n",
      "Train f1_micro: 0.8364565502950163\n",
      "\n",
      "Train f1_macro: 0.14639259749317832\n",
      "\n",
      "Train f1_weighted: 0.7733381622262931\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044e2d48d0a54f4db0eb76ae2037b8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.70144721912714\n",
      "\n",
      "Test accuracy: 0.8035784023569104\n",
      "\n",
      "Test precision_micro: 0.8035784023569104\n",
      "\n",
      "Test precision_macro: 0.48863954116450836\n",
      "\n",
      "Test precision_weighted: 0.715494174092724\n",
      "\n",
      "Test recall_micro: 0.8035784023569104\n",
      "\n",
      "Test recall_macro: 0.5325989093043301\n",
      "\n",
      "Test recall_weighted: 0.8035784023569104\n",
      "\n",
      "Test f1_micro: 0.8035784023569104\n",
      "\n",
      "Test f1_macro: 0.5026287447141989\n",
      "\n",
      "Test f1_weighted: 0.748645619952125\n",
      "\n",
      "Epoch [3 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8186f9fb8474391bf611567ca0e03a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4903683354245855\n",
      "\n",
      "Train accuracy: 0.8640806941036921\n",
      "\n",
      "Train precision_micro: 0.8640806941036921\n",
      "\n",
      "Train precision_macro: 0.4576155031363186\n",
      "\n",
      "Train precision_weighted: 0.8329937101380968\n",
      "\n",
      "Train recall_micro: 0.8640806941036921\n",
      "\n",
      "Train recall_macro: 0.3001478068564831\n",
      "\n",
      "Train recall_weighted: 0.8640806941036921\n",
      "\n",
      "Train f1_micro: 0.8640806941036921\n",
      "\n",
      "Train f1_macro: 0.33258961051451286\n",
      "\n",
      "Train f1_weighted: 0.8371320598901064\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546f9754c6f34db2b96549ba0b157eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ### START CODE HERE ###\n",
    "train(n_epochs=n_epoch,\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        valid_dataloader=valid_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        model_type=\"BiLSTM\")\n",
    "# ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrW7cODysJ4j"
   },
   "source": [
    "Здесь и далее проинициализируем *tensorboard* для логгирования метрики в процессе обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNIk2Rd5kjRa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13504), started 9:03:10 ago. (Use '!kill 13504' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En2_vpe7jTwG"
   },
   "source": [
    "Проверим качество на тестовой выборке, ожидаем `f1_macro >= 0.76`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-tOLEdbQ6gD"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff327bfbd9c4d4aaf968f779966c0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3683 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.38736979191838367\n",
      "\n",
      "Test accuracy: 0.9196830673302103\n",
      "\n",
      "Test precision_micro: 0.9196830673302103\n",
      "\n",
      "Test precision_macro: 0.8030628735119713\n",
      "\n",
      "Test precision_weighted: 0.9169418498779961\n",
      "\n",
      "Test recall_micro: 0.9196830673302103\n",
      "\n",
      "Test recall_macro: 0.8008357489974941\n",
      "\n",
      "Test recall_weighted: 0.9196830673302103\n",
      "\n",
      "Test f1_micro: 0.9196830673302103\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9141430941896991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_epoch(\n",
    "    model=model,\n",
    "    dataloader=test_dataloader,\n",
    "    criterion=criterion,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    epoch=1,\n",
    "    model_type='BiLSTM'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно было и 40 эпох запустить, тогда там под 0.9 будет, но мне лень ждать, задание я выполнил."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8R6nopyQEL-"
   },
   "source": [
    "## Часть 3. Transformers-теггер (6 баллов)\n",
    "\n",
    "В данной части задания нужно сделать все то же самое, но с использованием модели на базе архитектуры Transformer, а именно предлагается дообучать предобученную модель **BERT**.\n",
    "\n",
    "Для данной модели подразумевается специальная подготовка данных, с чего мы и начнем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrbX5gFDQEL-"
   },
   "source": [
    "Модель **BERT** использует специальный токенизатор WordPiece для разбиения предложений на токены. Готовая предобученная версия такого токенизатора существует в библиотеке **transformers**. Есть два класса: `BertTokenizer` и `BertTokenizerFast`. Использовать можно любой, но второй вариант работает существенно быстрее.\n",
    "\n",
    "Токенизаторы можно обучать с нуля на своем корпусе данных, а можно подгружать уже готовые. Готовые токенизаторы, как правило, соответствуют предобученной конфигурации модели, которая использует словарь из этого токенизатора.\n",
    "\n",
    "Мы будем использовать базовую конфигурацию предобученного **BERT** для модели и токенизатора.\n",
    "\n",
    "P.S. Часто приходится проводить эксперименты с моделями разной архитектуры, например **BERT** и **GPT**, поэтому удобно использовать класс `AutoTokenizer`, который по названию модели сам определит, какой класс нужен для инициализации токенизатора.\n",
    "\n",
    "Существует полезный сервис **HuggingFace**, который собрал в себе большое множество моделей и данных, ссылки на ресурс:\n",
    "- Hugging Face: https://huggingface.co\n",
    "- Hugging Face Models: https://huggingface.co/models\n",
    "- Hugging Face Datasets: https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "4-UTiI4gQEL-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "kSbBhvnDQEMA"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxWNX5i6QEMA"
   },
   "source": [
    "Подгружение предобученных моделей и токенизаторов в **huggingface** происходит с помощью конструктора **from_pretrained**.\n",
    "\n",
    "В данном конструкторе можно указать либо путь к предобученному токенизатору, либо название предобученной конфигурации, как в нашем случае: тогда **transformers** сам подгрузит нужные параметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "3tg_bCeaQEMA"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MIrbmNoQEMA"
   },
   "source": [
    "### Подготовка словарей\n",
    "\n",
    "В сравнении с рекуррентными моделями, нам больше не нужно заниматься сборкой словаря, так как это уже сделано заранее благодаря токенизаторам и алгоритмам, стоящими за ними.\n",
    "\n",
    "Но нам как и прежде потребуется:\n",
    "- {**label**}→{**label_idx**}: соответствие между тегом и уникальным индексом (начинается с 0);\n",
    "\n",
    "Но данное отображение у нас уже реализовано в одной из предыдущих частей задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvYF-4uaQEMB"
   },
   "source": [
    "### Подготовка датасета и загрузчика\n",
    "\n",
    "Мы также хотим обучать модель батчами, поэтому нам как и прежде понадобятся `Dataset`, `Collator` и `DataLoader`.\n",
    "\n",
    "Но мы не можем переиспользовать те, что в предыдущих частях задания, так как обработка данных должна производится немного иначе с использованием токенизатора.\n",
    "\n",
    "Давайте напишем новый кастомный датасет, который на вход (метод `__init__`) будет принимать:\n",
    "- token_seq - список списков слов / токенов\n",
    "- label_seq - список списков тегов\n",
    "\n",
    "и возвращать из метода `__getitem__` два списка:\n",
    "- список текстовых значений (`List[str]`) из индексов токенов в сэмпле\n",
    "- список целочисленных значений (`List[int]`) из индексов соответвующих тегов\n",
    "\n",
    "P.S. В отличие от предыдущего кастомного датасет, здесь мы возвращаем два `List`'а вместо `torch.LongTensor`, так как логику формирования западдированного батча мы перенесем в `Collator` из-за специфики работы токенизатора - он сам возвращает уже западдированный тензор с индексами токенов, а для индексов тегов нам нужно будет сделать это самостоятельно по аналогии с предыдущим датасетом.\n",
    "\n",
    "**Задание. Реализуйте класс датасета TransformersDataset.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "7EoNLDOOQEMB"
   },
   "outputs": [],
   "source": [
    "class TransformersDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Transformers Dataset for NER.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_seq: List[List[str]],\n",
    "        label_seq: List[List[str]],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "\n",
    "        Args:\n",
    "            token_seq: the list of lists contains token sequences.\n",
    "            label_seq: the list of lists consists of label sequences.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.token_seq = token_seq\n",
    "        self.label_seq = [self.process_labels(labels, label2idx) for labels in label_seq]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns length of the dataset.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.token_seq)\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "    ) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Gets one item for tthe dataset\n",
    "\n",
    "        Args:\n",
    "            idx: the index of the particular element in the dataset\n",
    "\n",
    "        Returns:\n",
    "            (tokens, labels), where `tokens` is sequence of token in the dataset\n",
    "                by index `idx` and `labels` is corresponding labels list\n",
    "        \"\"\"\n",
    "        tokens = self.token_seq[idx]\n",
    "        labels = self.label_seq[idx]\n",
    "\n",
    "        return tokens, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def process_labels(\n",
    "        labels: List[str],\n",
    "        label2idx: Dict[str, int],\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Transform list of labels into list of labels' indices.\n",
    "\n",
    "        Args:\n",
    "            labels: the list of strings contains the labels\n",
    "            label2idx: mapping from a label to an index\n",
    "\n",
    "        Returns:\n",
    "            ids: the sequence of indices that correspond to labels\n",
    "        \"\"\"\n",
    "\n",
    "        ids = [label2idx[label] for label in labels]\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1oNc-31QEMB"
   },
   "source": [
    "Создадим три датасета:\n",
    "- *train_dataset*\n",
    "- *valid_dataset*\n",
    "- *test_dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "vqg56Jf8QEMC"
   },
   "outputs": [],
   "source": [
    "train_dataset = TransformersDataset(\n",
    "    token_seq=train_token_seq,\n",
    "    label_seq=train_label_seq,\n",
    ")\n",
    "valid_dataset = TransformersDataset(\n",
    "    token_seq=valid_token_seq,\n",
    "    label_seq=valid_label_seq,\n",
    ")\n",
    "test_dataset = TransformersDataset(\n",
    "    token_seq=test_token_seq,\n",
    "    label_seq=test_label_seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdIS6XrvQEMC"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "IT00Pjy6QEMC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
       " [3, 0, 2, 0, 0, 0, 2, 0, 0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "uYal2icQmuD-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cricket',\n",
       "  '-',\n",
       "  'leicestershire',\n",
       "  'take',\n",
       "  'over',\n",
       "  'at',\n",
       "  'top',\n",
       "  'after',\n",
       "  'innings',\n",
       "  'victory',\n",
       "  '.'],\n",
       " [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "FCXd3FWVmuKe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['soccer',\n",
       "  '-',\n",
       "  'japan',\n",
       "  'get',\n",
       "  'lucky',\n",
       "  'win',\n",
       "  ',',\n",
       "  'china',\n",
       "  'in',\n",
       "  'surprise',\n",
       "  'defeat',\n",
       "  '.'],\n",
       " [0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B4R605vAnYT9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert len(train_dataset) == 14986, \"Неправильная длина train_dataset\"\n",
    "assert len(valid_dataset) == 3465, \"Неправильная длина valid_dataset\"\n",
    "assert len(test_dataset) == 3683, \"Неправильная длина test_dataset\"\n",
    "\n",
    "assert train_dataset[0][0] == ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'], \"Неправильно сформированный train_dataset\"\n",
    "assert train_dataset[0][1] == [3,0,2,0,0,0,2,0,0], \"Неправильно сформированный train_dataset\"\n",
    "\n",
    "assert valid_dataset[0][0] == ['cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top', 'after', 'innings', 'victory', '.'], \"Неправильно сформированный valid_dataset\"\n",
    "assert valid_dataset[0][1] == [0,0,3,0,0,0,0,0,0,0,0], \"Неправильно сформированный valid_dataset\"\n",
    "\n",
    "assert test_dataset[0][0] == ['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.'], \"Неправильно сформированный test_dataset\"\n",
    "assert test_dataset[0][1] == [0,0,1,0,0,0,0,4,0,0,0,0], \"Неправильно сформированный test_dataset\"\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zP_6iQnQEMC"
   },
   "source": [
    "Реализуем новый `Collator`.\n",
    "\n",
    "Инициализировать коллатор будет 3 аргументами:\n",
    "- токенизатор\n",
    "- параметры токенизатора в виде словаря (затем используем как `**kwargs`)\n",
    "- id спецтокена для последовательностей тегов (значение -1)\n",
    "\n",
    "Метод `__call__` на вход принимает батч, а именно список кортежей того, что нам возвращается из датасета. В нашем случае это список кортежей двух int64 тензоров - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
    "\n",
    "На выходе мы хотим получить два тензора:\n",
    "- западденные индексы слов / токенов\n",
    "- западденные индексы тегов\n",
    "\n",
    "**Задание. Реализуйте класс коллатора TransformersCollator.** **<font color='red'>(2 балла)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "BonAp65jQEMD"
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "\n",
    "class TransformersCollator:\n",
    "    \"\"\"\n",
    "    Transformers Collator that handles variable-size sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        tokenizer_kwargs: Dict[str, Any],\n",
    "        label_padding_value: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        TransformersCollator class constructor.\n",
    "\n",
    "        Args:\n",
    "            tokenizer: the pretrained tokenizer which converts sentence\n",
    "                to tokens.\n",
    "            tokenizer_kwargs: the arguments of the tokenizer\n",
    "            label_padding_value: the padding value for a label\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_kwargs = tokenizer_kwargs\n",
    "\n",
    "        self.label_padding_value = label_padding_value\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: List[Tuple[List[str], List[int]]],\n",
    "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        Calls transformers' collator.\n",
    "\n",
    "        Args:\n",
    "            batch: One batch with sentence and labels.\n",
    "\n",
    "        Returns:\n",
    "            (tokens, labels), where `tokens` is sequence of token\n",
    "                and `labels` is corresponding labels list\n",
    "        \"\"\"\n",
    "        tokens, labels = zip(*batch)\n",
    "\n",
    "        tokens = self.tokenizer(tokens, **self.tokenizer_kwargs)\n",
    "        labels = self.encode_labels(tokens, labels, self.label_padding_value)\n",
    "\n",
    "        tokens.pop(\"offset_mapping\")\n",
    "\n",
    "        return tokens, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_labels(\n",
    "        tokens: BatchEncoding,\n",
    "        labels: List[List[int]],\n",
    "        label_padding_value: int,\n",
    "    ) -> torch.LongTensor:\n",
    "\n",
    "        encoded_labels = []\n",
    "\n",
    "        for doc_labels, doc_offset in zip(labels, tokens.offset_mapping):\n",
    "\n",
    "            doc_enc_labels = np.ones(len(doc_offset), dtype=int) * label_padding_value\n",
    "            arr_offset = np.array(doc_offset)\n",
    "\n",
    "            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "            encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "        return torch.LongTensor(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "iC8JkUPnQEMD"
   },
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"is_split_into_words\":    True,\n",
    "    \"return_offsets_mapping\": True,\n",
    "    \"padding\":                True,\n",
    "    \"truncation\":             True,\n",
    "    \"max_length\":             512,\n",
    "    \"return_tensors\":         \"pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "5sCDaxR6QEMD"
   },
   "outputs": [],
   "source": [
    "collator = TransformersCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_kwargs=tokenizer_kwargs,\n",
    "    label_padding_value=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eirev0N_QEMD"
   },
   "source": [
    "Теперь всё готово, чтобы задать `DataLoader`'ы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "9JDrLC6pQEME"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3zGjEDHQEME"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "KSWcYEAWQEME"
   },
   "outputs": [],
   "source": [
    "tokens, labels = next(iter(train_dataloader))\n",
    "\n",
    "tokens = tokens.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "NTcdU1BlQEME"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   174,  4188,  1424,   125,   122,   122,   123,   127,   129,\n",
       "           125,   102,     0,     0,     0,     0],\n",
       "        [  101,   118,   118, 14247,  1548,  2371,  6077,   116,  3081,   122,\n",
       "          3565, 18202,  4335,  1571,  1477,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "p7ZTh97-QEME"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1,  3, -1, -1,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1],\n",
       "        [-1,  0, -1,  3, -1,  7, -1,  0, -1,  0,  0, -1,  0, -1, -1, -1]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "pMprtk9bodM9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(train_tokens['input_ids'], torch.tensor([[  101,   174,  1358, 22961,   176, 14170,  1840,  1106, 21423,  9304, 10721,  1324,  2495, 12913,   119,   102], [  101, 11109,  1200,  1602,  6715,   102,     0,     0,     0,     0,    0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(train_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(train_labels, torch.tensor([[-1,  3, -1,  0,  2, -1,  0,  0,  0,  2, -1, -1,  0, -1,  0, -1], [-1,  4, -1,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "valid_tokens, valid_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(valid_tokens['input_ids'], torch.tensor([[  101,  5428,   118,  5837, 18117,  5759, 15189,  1321,  1166,  1120,  1499,  1170,  6687,  2681,   119,   102], [  101, 25338, 17996,  1820,   118,  4775,   118,  1476,   102,     0,     0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(valid_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(valid_labels, torch.tensor([[-1,  0,  0,  3, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1], [-1,  1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "test_tokens, test_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(test_tokens['input_ids'], torch.tensor([[  101,  5862,   118,   179, 26519,  1179,  1243,  6918,  1782,   117,  5144,  1161,  1107,  3774,  3326,   119,   102], [  101,  9468,  3309,  1306, 19122,  2293,   102,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(test_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
    "assert torch.equal(test_labels, torch.tensor([[-1,  0,  0,  1, -1, -1,  0,  0,  0,  0,  4, -1,  0,  0,  0,  0, -1], [-1,  4, -1, -1,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m-taH0SQEMF"
   },
   "source": [
    "В библиотеке **transformers** есть классы для модели BERT, уже настроенные под решение конкретных задач, с соответствующими головами классификации. Для задачи NER будем использовать класс `BertForTokenClassification`.\n",
    "\n",
    "По аналогии с токенизаторами, мы можем использовать класс `AutoModelForTokenClassification`, который по названию модели сам определит, какой класс нужен для инициализации модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "x6tq_i7JQEMF"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Vma9yj0zQEMF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2idx),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Imv-6gAQQEMG"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "LAdHfn4oQEMG"
   },
   "outputs": [],
   "source": [
    "outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "P-kTke_8QEMG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert 2 < criterion(outputs[\"logits\"].transpose(1, 2), labels) < 3\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "5Ana4qGKeHrN"
   },
   "outputs": [],
   "source": [
    "# создадим SummaryWriter для эксперимента с BiLSTMModel\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"logs/Transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sNuFPRdQEMH"
   },
   "source": [
    "### Эксперименты\n",
    "\n",
    "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1-macro мере на валидационной и тестовой выборках было не меньше **0.9**.\n",
    "\n",
    "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IfkN20lrN0J"
   },
   "source": [
    "Вы можете использовать ту же самую функцию train, что и до этого за тем исключением, что вместо инференса `model(tokens)` нужно делать `model(**tokens)`, а вместо `outputs` использовать `outputs[\"logits\"].transpose(1, 2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iyZUFddzYE5"
   },
   "source": [
    "**Задание. Проведите эксперименты.** **<font color='red'>(2 балла)</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "jFW92sNL4YCA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2c3a5f71b546f580cbe268c1a80b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.118204983244551\n",
      "\n",
      "Train accuracy: 0.8147363747071531\n",
      "\n",
      "Train precision_micro: 0.8147363747071531\n",
      "\n",
      "Train precision_macro: 0.09354018717454288\n",
      "\n",
      "Train precision_weighted: 0.6913690101699004\n",
      "\n",
      "Train recall_micro: 0.8147363747071531\n",
      "\n",
      "Train recall_macro: 0.10975921916142607\n",
      "\n",
      "Train recall_weighted: 0.8147363747071531\n",
      "\n",
      "Train f1_micro: 0.8147363747071531\n",
      "\n",
      "Train f1_macro: 0.10066333931177197\n",
      "\n",
      "Train f1_weighted: 0.7472029149899261\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8f4fecc0fc472eb63df4e6d0ce7a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.8386772014754375\n",
      "\n",
      "Test accuracy: 0.7856210939550704\n",
      "\n",
      "Test precision_micro: 0.7856210939550704\n",
      "\n",
      "Test precision_macro: 0.4408485517270401\n",
      "\n",
      "Test precision_weighted: 0.6643203710677784\n",
      "\n",
      "Test recall_micro: 0.7856210939550704\n",
      "\n",
      "Test recall_macro: 0.5082354153782725\n",
      "\n",
      "Test recall_weighted: 0.7856210939550704\n",
      "\n",
      "Test f1_micro: 0.7856210939550704\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.7118999642853749\n",
      "\n",
      "Epoch [2 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc7d5dcaa1d494e9456739e74b2a18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5843476031688933\n",
      "\n",
      "Train accuracy: 0.8332213607848434\n",
      "\n",
      "Train precision_micro: 0.8332213607848434\n",
      "\n",
      "Train precision_macro: 0.11107315956985825\n",
      "\n",
      "Train precision_weighted: 0.7001827976782911\n",
      "\n",
      "Train recall_micro: 0.8332213607848434\n",
      "\n",
      "Train recall_macro: 0.11338709093688987\n",
      "\n",
      "Train recall_weighted: 0.8332213607848434\n",
      "\n",
      "Train f1_micro: 0.8332213607848434\n",
      "\n",
      "Train f1_macro: 0.10447217758377088\n",
      "\n",
      "Train f1_weighted: 0.7583381785652313\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceaffa51f6494640866ca2e539b37652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.6030414238394726\n",
      "\n",
      "Test accuracy: 0.7941671030491471\n",
      "\n",
      "Test precision_micro: 0.7941671030491471\n",
      "\n",
      "Test precision_macro: 0.46543648623354184\n",
      "\n",
      "Test precision_weighted: 0.6901968793237205\n",
      "\n",
      "Test recall_micro: 0.7941671030491471\n",
      "\n",
      "Test recall_macro: 0.5186544892499065\n",
      "\n",
      "Test recall_weighted: 0.7941671030491471\n",
      "\n",
      "Test f1_micro: 0.7941671030491471\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.7302980636536653\n",
      "\n",
      "Epoch [3 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e513d91779f1483a82f19461f56ed1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4486457366892632\n",
      "\n",
      "Train accuracy: 0.8646739029625148\n",
      "\n",
      "Train precision_micro: 0.8646739029625148\n",
      "\n",
      "Train precision_macro: 0.27510446543146017\n",
      "\n",
      "Train precision_weighted: 0.8297033913806522\n",
      "\n",
      "Train recall_micro: 0.8646739029625148\n",
      "\n",
      "Train recall_macro: 0.25961119597445736\n",
      "\n",
      "Train recall_weighted: 0.8646739029625148\n",
      "\n",
      "Train f1_micro: 0.8646739029625148\n",
      "\n",
      "Train f1_macro: 0.2379362885753007\n",
      "\n",
      "Train f1_weighted: 0.8387959528015109\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c177b60853442c2be435ebcc1537329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.46712799241301345\n",
      "\n",
      "Test accuracy: 0.8569183098608705\n",
      "\n",
      "Test precision_micro: 0.8569183098608705\n",
      "\n",
      "Test precision_macro: 0.5795783771991602\n",
      "\n",
      "Test precision_weighted: 0.8368203437815256\n",
      "\n",
      "Test recall_micro: 0.8569183098608705\n",
      "\n",
      "Test recall_macro: 0.613542484411862\n",
      "\n",
      "Test recall_weighted: 0.8569183098608705\n",
      "\n",
      "Test f1_micro: 0.8569183098608705\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.8413940553183202\n",
      "\n",
      "Epoch [4 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e00f50e1f604655a13d122384caf559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3479326645110516\n",
      "\n",
      "Train accuracy: 0.9008168911872221\n",
      "\n",
      "Train precision_micro: 0.9008168911872221\n",
      "\n",
      "Train precision_macro: 0.41150801669060666\n",
      "\n",
      "Train precision_weighted: 0.8817871428025997\n",
      "\n",
      "Train recall_micro: 0.9008168911872221\n",
      "\n",
      "Train recall_macro: 0.3819302512080916\n",
      "\n",
      "Train recall_weighted: 0.9008168911872221\n",
      "\n",
      "Train f1_micro: 0.9008168911872221\n",
      "\n",
      "Train f1_macro: 0.35295303122099286\n",
      "\n",
      "Train f1_weighted: 0.8813014979952665\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feef06bb7d054224b4328fac8cd7cd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.33599392553058305\n",
      "\n",
      "Test accuracy: 0.8992455246530316\n",
      "\n",
      "Test precision_micro: 0.8992455246530316\n",
      "\n",
      "Test precision_macro: 0.7228246844465611\n",
      "\n",
      "Test precision_weighted: 0.8894791184725945\n",
      "\n",
      "Test recall_micro: 0.8992455246530316\n",
      "\n",
      "Test recall_macro: 0.7311060987093092\n",
      "\n",
      "Test recall_weighted: 0.8992455246530316\n",
      "\n",
      "Test f1_micro: 0.8992455246530316\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.8900799956456611\n",
      "\n",
      "Epoch [5 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec9df2ef4164e848aa9163aae44c7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2498393606632314\n",
      "\n",
      "Train accuracy: 0.9315986227711904\n",
      "\n",
      "Train precision_micro: 0.9315986227711904\n",
      "\n",
      "Train precision_macro: 0.5653917622669888\n",
      "\n",
      "Train precision_weighted: 0.9175232808983117\n",
      "\n",
      "Train recall_micro: 0.9315986227711904\n",
      "\n",
      "Train recall_macro: 0.5207492374069074\n",
      "\n",
      "Train recall_weighted: 0.9315986227711904\n",
      "\n",
      "Train f1_micro: 0.9315986227711904\n",
      "\n",
      "Train f1_macro: 0.5161972393100787\n",
      "\n",
      "Train f1_weighted: 0.9196524214359538\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563fc78e150141e496777d192129bd34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.23643302303108268\n",
      "\n",
      "Test accuracy: 0.9284177769479007\n",
      "\n",
      "Test precision_micro: 0.9284177769479007\n",
      "\n",
      "Test precision_macro: 0.7921671348581932\n",
      "\n",
      "Test precision_weighted: 0.929009498652267\n",
      "\n",
      "Test recall_micro: 0.9284177769479007\n",
      "\n",
      "Test recall_macro: 0.7965862583923284\n",
      "\n",
      "Test recall_weighted: 0.9284177769479007\n",
      "\n",
      "Test f1_micro: 0.9284177769479007\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9248346379299469\n",
      "\n",
      "Epoch [6 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753218ce09d24f6db4cf8ecd0812eb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1806941070613709\n",
      "\n",
      "Train accuracy: 0.9496513029977975\n",
      "\n",
      "Train precision_micro: 0.9496513029977975\n",
      "\n",
      "Train precision_macro: 0.6764534510467096\n",
      "\n",
      "Train precision_weighted: 0.9438487294806076\n",
      "\n",
      "Train recall_micro: 0.9496513029977975\n",
      "\n",
      "Train recall_macro: 0.6350359598421026\n",
      "\n",
      "Train recall_weighted: 0.9496513029977975\n",
      "\n",
      "Train f1_micro: 0.9496513029977975\n",
      "\n",
      "Train f1_macro: 0.6363506292198964\n",
      "\n",
      "Train f1_weighted: 0.9443619609440996\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a95fe5fc164b78b0fa595b2ff5519c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.1752714841429703\n",
      "\n",
      "Test accuracy: 0.9488415825239741\n",
      "\n",
      "Test precision_micro: 0.9488415825239741\n",
      "\n",
      "Test precision_macro: 0.8431908712652725\n",
      "\n",
      "Test precision_weighted: 0.952631377631596\n",
      "\n",
      "Test recall_micro: 0.9488415825239741\n",
      "\n",
      "Test recall_macro: 0.8430789716270348\n",
      "\n",
      "Test recall_weighted: 0.9488415825239741\n",
      "\n",
      "Test f1_micro: 0.9488415825239741\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9479368380215586\n",
      "\n",
      "Epoch [7 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1669df0b7149bbbe68a87feb2d3eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.13716428910798215\n",
      "\n",
      "Train accuracy: 0.9624762315626093\n",
      "\n",
      "Train precision_micro: 0.9624762315626093\n",
      "\n",
      "Train precision_macro: 0.8203819133163421\n",
      "\n",
      "Train precision_weighted: 0.9629507057298882\n",
      "\n",
      "Train recall_micro: 0.9624762315626093\n",
      "\n",
      "Train recall_macro: 0.7524465598818075\n",
      "\n",
      "Train recall_weighted: 0.9624762315626093\n",
      "\n",
      "Train f1_micro: 0.9624762315626093\n",
      "\n",
      "Train f1_macro: 0.7637088820387931\n",
      "\n",
      "Train f1_weighted: 0.9604262832787532\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472e5b7874934f859e048b017b38bf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.1441388472227963\n",
      "\n",
      "Test accuracy: 0.9589607352822558\n",
      "\n",
      "Test precision_micro: 0.9589607352822558\n",
      "\n",
      "Test precision_macro: 0.8680326033090339\n",
      "\n",
      "Test precision_weighted: 0.9557716868579967\n",
      "\n",
      "Test recall_micro: 0.9589607352822558\n",
      "\n",
      "Test recall_macro: 0.8682318415249966\n",
      "\n",
      "Test recall_weighted: 0.9589607352822558\n",
      "\n",
      "Test f1_micro: 0.9589607352822558\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9548540827009079\n",
      "\n",
      "Epoch [8 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3c4b7254ec4c7aa49f0504309e9f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.11185165597720349\n",
      "\n",
      "Train accuracy: 0.9695038194325973\n",
      "\n",
      "Train precision_micro: 0.9695038194325973\n",
      "\n",
      "Train precision_macro: 0.8359591552730525\n",
      "\n",
      "Train precision_weighted: 0.9708276903014855\n",
      "\n",
      "Train recall_micro: 0.9695038194325973\n",
      "\n",
      "Train recall_macro: 0.8205782343579595\n",
      "\n",
      "Train recall_weighted: 0.9695038194325973\n",
      "\n",
      "Train f1_micro: 0.9695038194325973\n",
      "\n",
      "Train f1_macro: 0.8162896976282471\n",
      "\n",
      "Train f1_weighted: 0.9689686467666284\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef2975f47da4ebe9a4a383763cb53a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.1174617818623081\n",
      "\n",
      "Test accuracy: 0.9679496663007486\n",
      "\n",
      "Test precision_micro: 0.9679496663007486\n",
      "\n",
      "Test precision_macro: 0.8934459850044343\n",
      "\n",
      "Test precision_weighted: 0.9673307267071433\n",
      "\n",
      "Test recall_micro: 0.9679496663007486\n",
      "\n",
      "Test recall_macro: 0.8936377534715176\n",
      "\n",
      "Test recall_weighted: 0.9679496663007486\n",
      "\n",
      "Test f1_micro: 0.9679496663007486\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9657153508405969\n",
      "\n",
      "Epoch [9 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcf523996af481ea4a492cf7a7dbc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09448164140290402\n",
      "\n",
      "Train accuracy: 0.9738240891179324\n",
      "\n",
      "Train precision_micro: 0.9738240891179324\n",
      "\n",
      "Train precision_macro: 0.8725392415061273\n",
      "\n",
      "Train precision_weighted: 0.9747807848733993\n",
      "\n",
      "Train recall_micro: 0.9738240891179324\n",
      "\n",
      "Train recall_macro: 0.8411798070215107\n",
      "\n",
      "Train recall_weighted: 0.9738240891179324\n",
      "\n",
      "Train f1_micro: 0.9738240891179324\n",
      "\n",
      "Train f1_macro: 0.8451822029650566\n",
      "\n",
      "Train f1_weighted: 0.9730270270163592\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67defb98f24b4628b4f97c05ea987026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.10158825733326476\n",
      "\n",
      "Test accuracy: 0.9700722638331944\n",
      "\n",
      "Test precision_micro: 0.9700722638331944\n",
      "\n",
      "Test precision_macro: 0.9035627651638501\n",
      "\n",
      "Test precision_weighted: 0.9739708711472743\n",
      "\n",
      "Test recall_micro: 0.9700722638331944\n",
      "\n",
      "Test recall_macro: 0.9035686482004918\n",
      "\n",
      "Test recall_weighted: 0.9700722638331944\n",
      "\n",
      "Test f1_micro: 0.9700722638331944\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9700441506926546\n",
      "\n",
      "Epoch [10 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73372d27796942419ea125a942845bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08078522094862259\n",
      "\n",
      "Train accuracy: 0.9790280861772565\n",
      "\n",
      "Train precision_micro: 0.9790280861772565\n",
      "\n",
      "Train precision_macro: 0.8913435588662433\n",
      "\n",
      "Train precision_weighted: 0.9804201868858534\n",
      "\n",
      "Train recall_micro: 0.9790280861772565\n",
      "\n",
      "Train recall_macro: 0.8708075654671434\n",
      "\n",
      "Train recall_weighted: 0.9790280861772565\n",
      "\n",
      "Train f1_micro: 0.9790280861772565\n",
      "\n",
      "Train f1_macro: 0.8704210142947799\n",
      "\n",
      "Train f1_weighted: 0.9786475346710201\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5147e7ecd54e1f874230c6f72420b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.09854848450030859\n",
      "\n",
      "Test accuracy: 0.9755621633119064\n",
      "\n",
      "Test precision_micro: 0.9755621633119064\n",
      "\n",
      "Test precision_macro: 0.9169911608295763\n",
      "\n",
      "Test precision_weighted: 0.9729277341641667\n",
      "\n",
      "Test recall_micro: 0.9755621633119064\n",
      "\n",
      "Test recall_macro: 0.9169117260538442\n",
      "\n",
      "Test recall_weighted: 0.9755621633119064\n",
      "\n",
      "Test f1_micro: 0.9755621633119064\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9726713445136322\n",
      "\n",
      "Epoch [11 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd282fd7aa64ff0b975835bf25d7ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06987692550458807\n",
      "\n",
      "Train accuracy: 0.9817104768165709\n",
      "\n",
      "Train precision_micro: 0.9817104768165709\n",
      "\n",
      "Train precision_macro: 0.8966462149618444\n",
      "\n",
      "Train precision_weighted: 0.9832270980950628\n",
      "\n",
      "Train recall_micro: 0.9817104768165709\n",
      "\n",
      "Train recall_macro: 0.8964397453239157\n",
      "\n",
      "Train recall_weighted: 0.9817104768165709\n",
      "\n",
      "Train f1_micro: 0.9817104768165709\n",
      "\n",
      "Train f1_macro: 0.887588049228571\n",
      "\n",
      "Train f1_weighted: 0.9816351213857171\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d234988a9a648bbabadeaa072934d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.09109677728425532\n",
      "\n",
      "Test accuracy: 0.975540775858959\n",
      "\n",
      "Test precision_micro: 0.975540775858959\n",
      "\n",
      "Test precision_macro: 0.9181629915243399\n",
      "\n",
      "Test precision_weighted: 0.9787641933152141\n",
      "\n",
      "Test recall_micro: 0.975540775858959\n",
      "\n",
      "Test recall_macro: 0.9181136441614813\n",
      "\n",
      "Test recall_weighted: 0.975540775858959\n",
      "\n",
      "Test f1_micro: 0.975540775858959\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9755567786071452\n",
      "\n",
      "Epoch [12 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e9fbeeef45416d8eb3aeb6c757c535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05948819208969461\n",
      "\n",
      "Train accuracy: 0.9858997768569452\n",
      "\n",
      "Train precision_micro: 0.9858997768569452\n",
      "\n",
      "Train precision_macro: 0.9252352051684444\n",
      "\n",
      "Train precision_weighted: 0.9868381167167859\n",
      "\n",
      "Train recall_micro: 0.9858997768569452\n",
      "\n",
      "Train recall_macro: 0.9213330482438681\n",
      "\n",
      "Train recall_weighted: 0.9858997768569452\n",
      "\n",
      "Train f1_micro: 0.9858997768569452\n",
      "\n",
      "Train f1_macro: 0.916637867339053\n",
      "\n",
      "Train f1_weighted: 0.9857812243479768\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c53c4c80254cb39fc518868207a66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.09367715098729582\n",
      "\n",
      "Test accuracy: 0.9771585123495\n",
      "\n",
      "Test precision_micro: 0.9771585123495\n",
      "\n",
      "Test precision_macro: 0.9188141347130263\n",
      "\n",
      "Test precision_weighted: 0.976745177921311\n",
      "\n",
      "Test recall_micro: 0.9771585123495\n",
      "\n",
      "Test recall_macro: 0.9190058165155455\n",
      "\n",
      "Test recall_weighted: 0.9771585123495\n",
      "\n",
      "Test f1_micro: 0.9771585123495\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9753989820379141\n",
      "\n",
      "Epoch [13 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2f28301fb54e669bc0a5ea5c05956a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04673363651604728\n",
      "\n",
      "Train accuracy: 0.988862944838188\n",
      "\n",
      "Train precision_micro: 0.988862944838188\n",
      "\n",
      "Train precision_macro: 0.9398333637448352\n",
      "\n",
      "Train precision_weighted: 0.989667324127701\n",
      "\n",
      "Train recall_micro: 0.988862944838188\n",
      "\n",
      "Train recall_macro: 0.9382699828170245\n",
      "\n",
      "Train recall_weighted: 0.988862944838188\n",
      "\n",
      "Train f1_micro: 0.988862944838188\n",
      "\n",
      "Train f1_macro: 0.9338421217170324\n",
      "\n",
      "Train f1_weighted: 0.9888280593040265\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca61da43149d46fb86c80d9fef989095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.10213230363629448\n",
      "\n",
      "Test accuracy: 0.9734721150484797\n",
      "\n",
      "Test precision_micro: 0.9734721150484797\n",
      "\n",
      "Test precision_macro: 0.9134563233155888\n",
      "\n",
      "Test precision_weighted: 0.97626245649025\n",
      "\n",
      "Test recall_micro: 0.9734721150484797\n",
      "\n",
      "Test recall_macro: 0.9122902077494611\n",
      "\n",
      "Test recall_weighted: 0.9734721150484797\n",
      "\n",
      "Test f1_micro: 0.9734721150484797\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9731531779201156\n",
      "\n",
      "Epoch [14 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6719d629941e48a28cb06e278fc0d7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04251516657940885\n",
      "\n",
      "Train accuracy: 0.9899588237824359\n",
      "\n",
      "Train precision_micro: 0.9899588237824359\n",
      "\n",
      "Train precision_macro: 0.9482693808358895\n",
      "\n",
      "Train precision_weighted: 0.9906138396848422\n",
      "\n",
      "Train recall_micro: 0.9899588237824359\n",
      "\n",
      "Train recall_macro: 0.9430253254167716\n",
      "\n",
      "Train recall_weighted: 0.9899588237824359\n",
      "\n",
      "Train f1_micro: 0.9899588237824359\n",
      "\n",
      "Train f1_macro: 0.9416965355561939\n",
      "\n",
      "Train f1_weighted: 0.9898881974308247\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5d7e0ba82d4a9fb9607e72b49bb8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.08353900175582292\n",
      "\n",
      "Test accuracy: 0.9810520288100751\n",
      "\n",
      "Test precision_micro: 0.9810520288100751\n",
      "\n",
      "Test precision_macro: 0.9330010859384111\n",
      "\n",
      "Test precision_weighted: 0.9807004548654957\n",
      "\n",
      "Test recall_micro: 0.9810520288100751\n",
      "\n",
      "Test recall_macro: 0.9326356413318698\n",
      "\n",
      "Test recall_weighted: 0.9810520288100751\n",
      "\n",
      "Test f1_micro: 0.9810520288100751\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9797244847206701\n",
      "\n",
      "Epoch [15 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504253377a1d4f26ab3973c323a0d686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03509458982326249\n",
      "\n",
      "Train accuracy: 0.9921198863823152\n",
      "\n",
      "Train precision_micro: 0.9921198863823152\n",
      "\n",
      "Train precision_macro: 0.9591116439714976\n",
      "\n",
      "Train precision_weighted: 0.9926388005834076\n",
      "\n",
      "Train recall_micro: 0.9921198863823152\n",
      "\n",
      "Train recall_macro: 0.9610484744647416\n",
      "\n",
      "Train recall_weighted: 0.9921198863823152\n",
      "\n",
      "Train f1_micro: 0.9921198863823152\n",
      "\n",
      "Train f1_macro: 0.9567235226440888\n",
      "\n",
      "Train f1_weighted: 0.992099973144203\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d7f026903f4ddd9af7ba6f4a48cb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.0920708384732904\n",
      "\n",
      "Test accuracy: 0.9799400863253401\n",
      "\n",
      "Test precision_micro: 0.9799400863253401\n",
      "\n",
      "Test precision_macro: 0.9273132611968091\n",
      "\n",
      "Test precision_weighted: 0.9817957707896479\n",
      "\n",
      "Test recall_micro: 0.9799400863253401\n",
      "\n",
      "Test recall_macro: 0.9278603343691615\n",
      "\n",
      "Test recall_weighted: 0.9799400863253401\n",
      "\n",
      "Test f1_micro: 0.9799400863253401\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9796094352286076\n",
      "\n",
      "Epoch [16 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ec19f9d68f405894624bae7dbb1a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02997692663896274\n",
      "\n",
      "Train accuracy: 0.9938464535388963\n",
      "\n",
      "Train precision_micro: 0.9938464535388963\n",
      "\n",
      "Train precision_macro: 0.9671108049487415\n",
      "\n",
      "Train precision_weighted: 0.994227779208559\n",
      "\n",
      "Train recall_micro: 0.9938464535388963\n",
      "\n",
      "Train recall_macro: 0.9671886910488768\n",
      "\n",
      "Train recall_weighted: 0.9938464535388963\n",
      "\n",
      "Train f1_micro: 0.9938464535388963\n",
      "\n",
      "Train f1_macro: 0.9645461863753453\n",
      "\n",
      "Train f1_weighted: 0.9938371617471505\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc584c9d60444adb634f8c288f343d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.08788699824073634\n",
      "\n",
      "Test accuracy: 0.9818976131335196\n",
      "\n",
      "Test precision_micro: 0.9818976131335196\n",
      "\n",
      "Test precision_macro: 0.9359747336261465\n",
      "\n",
      "Test precision_weighted: 0.9820036807128176\n",
      "\n",
      "Test recall_micro: 0.9818976131335196\n",
      "\n",
      "Test recall_macro: 0.9352244061136953\n",
      "\n",
      "Test recall_weighted: 0.9818976131335196\n",
      "\n",
      "Test f1_micro: 0.9818976131335196\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9807862550760847\n",
      "\n",
      "Epoch [17 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2018450aa745e7a127c248f345afb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.023178633738388407\n",
      "\n",
      "Train accuracy: 0.9951744445254046\n",
      "\n",
      "Train precision_micro: 0.9951744445254046\n",
      "\n",
      "Train precision_macro: 0.9712341105298108\n",
      "\n",
      "Train precision_weighted: 0.9954809033543817\n",
      "\n",
      "Train recall_micro: 0.9951744445254046\n",
      "\n",
      "Train recall_macro: 0.9755362682488263\n",
      "\n",
      "Train recall_weighted: 0.9951744445254046\n",
      "\n",
      "Train f1_micro: 0.9951744445254046\n",
      "\n",
      "Train f1_macro: 0.9714989742837429\n",
      "\n",
      "Train f1_weighted: 0.9951778116756369\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e73c0e9b2684a5eb1daf1c8558a6d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.10326842674122912\n",
      "\n",
      "Test accuracy: 0.9775023051755172\n",
      "\n",
      "Test precision_micro: 0.9775023051755172\n",
      "\n",
      "Test precision_macro: 0.922214460042795\n",
      "\n",
      "Test precision_weighted: 0.9805315554856775\n",
      "\n",
      "Test recall_micro: 0.9775023051755172\n",
      "\n",
      "Test recall_macro: 0.9212225771128304\n",
      "\n",
      "Test recall_weighted: 0.9775023051755172\n",
      "\n",
      "Test f1_micro: 0.9775023051755172\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9776235117623583\n",
      "\n",
      "Epoch [18 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297ad13b238b4b12ba0f02bfb6ad0c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.022792498747560574\n",
      "\n",
      "Train accuracy: 0.9955715116095766\n",
      "\n",
      "Train precision_micro: 0.9955715116095766\n",
      "\n",
      "Train precision_macro: 0.9742932821033344\n",
      "\n",
      "Train precision_weighted: 0.9958960582925687\n",
      "\n",
      "Train recall_micro: 0.9955715116095766\n",
      "\n",
      "Train recall_macro: 0.9766131453065839\n",
      "\n",
      "Train recall_weighted: 0.9955715116095766\n",
      "\n",
      "Train f1_micro: 0.9955715116095766\n",
      "\n",
      "Train f1_macro: 0.9738414698876029\n",
      "\n",
      "Train f1_weighted: 0.9955944864850566\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40ce0c2e6984d778b31ed25d6450615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.09547202174517894\n",
      "\n",
      "Test accuracy: 0.9794174388043222\n",
      "\n",
      "Test precision_micro: 0.9794174388043222\n",
      "\n",
      "Test precision_macro: 0.9302866610864092\n",
      "\n",
      "Test precision_weighted: 0.9803606742666489\n",
      "\n",
      "Test recall_micro: 0.9794174388043222\n",
      "\n",
      "Test recall_macro: 0.9283709440726862\n",
      "\n",
      "Test recall_weighted: 0.9794174388043222\n",
      "\n",
      "Test f1_micro: 0.9794174388043222\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9785405948892688\n",
      "\n",
      "Epoch [19 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9202954e72f468f996a08cfdab37b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.01619160994788275\n",
      "\n",
      "Train accuracy: 0.9971092623857941\n",
      "\n",
      "Train precision_micro: 0.9971092623857941\n",
      "\n",
      "Train precision_macro: 0.9848691725994547\n",
      "\n",
      "Train precision_weighted: 0.9972657932272583\n",
      "\n",
      "Train recall_micro: 0.9971092623857941\n",
      "\n",
      "Train recall_macro: 0.9857757799613577\n",
      "\n",
      "Train recall_weighted: 0.9971092623857941\n",
      "\n",
      "Train f1_micro: 0.9971092623857941\n",
      "\n",
      "Train f1_macro: 0.9842075460862797\n",
      "\n",
      "Train f1_weighted: 0.997103800225374\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0762f7b60e42dcbf98fe8d3430f5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.08981330042061043\n",
      "\n",
      "Test accuracy: 0.9825385747647657\n",
      "\n",
      "Test precision_micro: 0.9825385747647657\n",
      "\n",
      "Test precision_macro: 0.9381769091103651\n",
      "\n",
      "Test precision_weighted: 0.9836130978013852\n",
      "\n",
      "Test recall_micro: 0.9825385747647657\n",
      "\n",
      "Test recall_macro: 0.937662404665564\n",
      "\n",
      "Test recall_weighted: 0.9825385747647657\n",
      "\n",
      "Test f1_micro: 0.9825385747647657\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9818969578121217\n",
      "\n",
      "Epoch [20 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2b82478a17404b943f55edbb45f745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.014858484667963646\n",
      "\n",
      "Train accuracy: 0.9972084928996362\n",
      "\n",
      "Train precision_micro: 0.9972084928996362\n",
      "\n",
      "Train precision_macro: 0.983617008437519\n",
      "\n",
      "Train precision_weighted: 0.9974136587653866\n",
      "\n",
      "Train recall_micro: 0.9972084928996362\n",
      "\n",
      "Train recall_macro: 0.9872248083476945\n",
      "\n",
      "Train recall_weighted: 0.9972084928996362\n",
      "\n",
      "Train f1_micro: 0.9972084928996362\n",
      "\n",
      "Train f1_macro: 0.9842305500830889\n",
      "\n",
      "Train f1_weighted: 0.9972247996093622\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd444d7987c463e906f0a667d5ed9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.09618224099804965\n",
      "\n",
      "Test accuracy: 0.9815093234331056\n",
      "\n",
      "Test precision_micro: 0.9815093234331056\n",
      "\n",
      "Test precision_macro: 0.9346390158411586\n",
      "\n",
      "Test precision_weighted: 0.9832594580725221\n",
      "\n",
      "Test recall_micro: 0.9815093234331056\n",
      "\n",
      "Test recall_macro: 0.9350107974308213\n",
      "\n",
      "Test recall_weighted: 0.9815093234331056\n",
      "\n",
      "Test f1_micro: 0.9815093234331056\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9811659030155367\n",
      "\n",
      "Epoch [21 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44670ae9c6e4da4a1286fb256076600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.013442052180006941\n",
      "\n",
      "Train accuracy: 0.9978743059004309\n",
      "\n",
      "Train precision_micro: 0.9978743059004309\n",
      "\n",
      "Train precision_macro: 0.9890786188662826\n",
      "\n",
      "Train precision_weighted: 0.9979903221068082\n",
      "\n",
      "Train recall_micro: 0.9978743059004309\n",
      "\n",
      "Train recall_macro: 0.9901740494553896\n",
      "\n",
      "Train recall_weighted: 0.9978743059004309\n",
      "\n",
      "Train f1_micro: 0.9978743059004309\n",
      "\n",
      "Train f1_macro: 0.9889180766183665\n",
      "\n",
      "Train f1_weighted: 0.9978763733108491\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68905426676d42c1b0e3832e1c62d6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.10211242406452135\n",
      "\n",
      "Test accuracy: 0.9822213416607585\n",
      "\n",
      "Test precision_micro: 0.9822213416607585\n",
      "\n",
      "Test precision_macro: 0.9379947696927602\n",
      "\n",
      "Test precision_weighted: 0.9819102918963257\n",
      "\n",
      "Test recall_micro: 0.9822213416607585\n",
      "\n",
      "Test recall_macro: 0.9381791962942441\n",
      "\n",
      "Test recall_weighted: 0.9822213416607585\n",
      "\n",
      "Test f1_micro: 0.9822213416607585\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9809306725551545\n",
      "\n",
      "Epoch [22 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb96341507345cdb46a1a7f45a06f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.01073637887602672\n",
      "\n",
      "Train accuracy: 0.9983261589342955\n",
      "\n",
      "Train precision_micro: 0.9983261589342955\n",
      "\n",
      "Train precision_macro: 0.9906416817009943\n",
      "\n",
      "Train precision_weighted: 0.9984204015662698\n",
      "\n",
      "Train recall_micro: 0.9983261589342955\n",
      "\n",
      "Train recall_macro: 0.9919678641431223\n",
      "\n",
      "Train recall_weighted: 0.9983261589342955\n",
      "\n",
      "Train f1_micro: 0.9983261589342955\n",
      "\n",
      "Train f1_macro: 0.990790872405464\n",
      "\n",
      "Train f1_weighted: 0.9983272595953689\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ff8c9eebed4110b6b3a50ba166860f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.10477642822396017\n",
      "\n",
      "Test accuracy: 0.9809205844972064\n",
      "\n",
      "Test precision_micro: 0.9809205844972064\n",
      "\n",
      "Test precision_macro: 0.9337383402464188\n",
      "\n",
      "Test precision_weighted: 0.9820941738909088\n",
      "\n",
      "Test recall_micro: 0.9809205844972064\n",
      "\n",
      "Test recall_macro: 0.9343498361669315\n",
      "\n",
      "Test recall_weighted: 0.9809205844972064\n",
      "\n",
      "Test f1_micro: 0.9809205844972064\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9803579333524615\n",
      "\n",
      "Epoch [23 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2582e71709f44025b9095d9102c28688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.009662517771976901\n",
      "\n",
      "Train accuracy: 0.9985568127716067\n",
      "\n",
      "Train precision_micro: 0.9985568127716067\n",
      "\n",
      "Train precision_macro: 0.9911414707759039\n",
      "\n",
      "Train precision_weighted: 0.9986618552024914\n",
      "\n",
      "Train recall_micro: 0.9985568127716067\n",
      "\n",
      "Train recall_macro: 0.9938801227818522\n",
      "\n",
      "Train recall_weighted: 0.9985568127716067\n",
      "\n",
      "Train f1_micro: 0.9985568127716067\n",
      "\n",
      "Train f1_macro: 0.9920476952495464\n",
      "\n",
      "Train f1_weighted: 0.9985711959116018\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3c7b89de9443758edea8961274c8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.11391971990664805\n",
      "\n",
      "Test accuracy: 0.980646996474259\n",
      "\n",
      "Test precision_micro: 0.980646996474259\n",
      "\n",
      "Test precision_macro: 0.9332589184787137\n",
      "\n",
      "Test precision_weighted: 0.9815117031928007\n",
      "\n",
      "Test recall_micro: 0.980646996474259\n",
      "\n",
      "Test recall_macro: 0.9330875150479254\n",
      "\n",
      "Test recall_weighted: 0.980646996474259\n",
      "\n",
      "Test f1_micro: 0.980646996474259\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9798514320800242\n",
      "\n",
      "Epoch [24 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efdd54af67a42ab85ca750d184d917f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.009172189443748683\n",
      "\n",
      "Train accuracy: 0.9988226203041295\n",
      "\n",
      "Train precision_micro: 0.9988226203041295\n",
      "\n",
      "Train precision_macro: 0.9931985543398062\n",
      "\n",
      "Train precision_weighted: 0.9988937216731405\n",
      "\n",
      "Train recall_micro: 0.9988226203041295\n",
      "\n",
      "Train recall_macro: 0.994527239135981\n",
      "\n",
      "Train recall_weighted: 0.9988226203041295\n",
      "\n",
      "Train f1_micro: 0.9988226203041295\n",
      "\n",
      "Train f1_macro: 0.9934059668705617\n",
      "\n",
      "Train f1_weighted: 0.9988273339133711\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3d9fd2a8bc48dc925d0f4ee33fe839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.1159805094196669\n",
      "\n",
      "Test accuracy: 0.9803757605540471\n",
      "\n",
      "Test precision_micro: 0.9803757605540471\n",
      "\n",
      "Test precision_macro: 0.9323493302047079\n",
      "\n",
      "Test precision_weighted: 0.9809245916890462\n",
      "\n",
      "Test recall_micro: 0.9803757605540471\n",
      "\n",
      "Test recall_macro: 0.9316222468492028\n",
      "\n",
      "Test recall_weighted: 0.9803757605540471\n",
      "\n",
      "Test f1_micro: 0.9803757605540471\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9793770668702999\n",
      "\n",
      "Epoch [25 / 25]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be45184dfc044d6ad903abed13fcaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.007650640017868198\n",
      "\n",
      "Train accuracy: 0.9990003591707759\n",
      "\n",
      "Train precision_micro: 0.9990003591707759\n",
      "\n",
      "Train precision_macro: 0.9937812779908037\n",
      "\n",
      "Train precision_weighted: 0.9990674088146497\n",
      "\n",
      "Train recall_micro: 0.9990003591707759\n",
      "\n",
      "Train recall_macro: 0.9955606533454896\n",
      "\n",
      "Train recall_weighted: 0.9990003591707759\n",
      "\n",
      "Train f1_micro: 0.9990003591707759\n",
      "\n",
      "Train f1_macro: 0.9943754159556012\n",
      "\n",
      "Train f1_weighted: 0.999007995497393\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f855663b52bd44a2a56ba9c7ec56d93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.11422236291773764\n",
      "\n",
      "Test accuracy: 0.9799248732047522\n",
      "\n",
      "Test precision_micro: 0.9799248732047522\n",
      "\n",
      "Test precision_macro: 0.9309828521025553\n",
      "\n",
      "Test precision_weighted: 0.9832336693038919\n",
      "\n",
      "Test recall_micro: 0.9799248732047522\n",
      "\n",
      "Test recall_macro: 0.9310777984765939\n",
      "\n",
      "Test recall_weighted: 0.9799248732047522\n",
      "\n",
      "Test f1_micro: 0.9799248732047522\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9802366068038202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ### START CODE HERE ###\n",
    "# Реализуйте ветку elif в функции train, которая\n",
    "# отвечает условию model_type == 'Transformer'\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "lr = 1e-6\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "n_epoch = 25\n",
    "train(n_epochs=n_epoch,\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        valid_dataloader=valid_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        model_type=\"Transformer\")\n",
    "# ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "ad4qt1dy6NuD"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43dfe16c6aa40ce9ff19c5602254095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over test batches:   0%|          | 0/3683 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.2827504105557959\n",
      "\n",
      "Test accuracy: 0.9578258204859789\n",
      "\n",
      "Test precision_micro: 0.9578258204859789\n",
      "\n",
      "Test precision_macro: 0.8960906042380917\n",
      "\n",
      "Test precision_weighted: 0.9650289739494925\n",
      "\n",
      "Test recall_micro: 0.9578258204859789\n",
      "\n",
      "Test recall_macro: 0.8951806688837194\n",
      "\n",
      "Test recall_weighted: 0.9578258204859789\n",
      "\n",
      "Test f1_micro: 0.9578258204859789\n",
      "\n",
      "Test f1_macro: 0.9091291513069883\n",
      "\n",
      "Test f1_weighted: 0.9596009781925166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_epoch(\n",
    "    model=model,\n",
    "    dataloader=test_dataloader,\n",
    "    criterion=criterion,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    epoch=1,\n",
    "    model_type='Transformer',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XlI3cb1QEL2"
   },
   "source": [
    "## Часть 4. Бонусы.\n",
    "\n",
    "## Бонус 1: BiLSTMAttention-теггер (2 баллa)\n",
    "\n",
    "Необходимо провести те же самые эксперименты как и в части 2, но уже с использованием усовершенствованной архитектуры теггера BiLSTM с Attention механизмом.\n",
    "\n",
    "**Обратите внимание**, что реализовывать Attention самому не нужно, можно использовать `torch.nn.MultiheadAttention`.\n",
    "\n",
    "Также сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров и проведите небольшой сравнительный анализ с предыдущей архитектурой. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook).\n",
    "\n",
    "**Задание. Реализуйте класс модели BiLSTMAttn.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MyLQp047yID"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Embedding, LSTM, Linear, MultiheadAttention\n",
    "\n",
    "\n",
    "class BiLSTMAttn(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        bidirectional: bool,\n",
    "        n_classes: int,\n",
    "        token_padding_value: int,\n",
    "        max_norm: float,\n",
    "        num_heads: int,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_padding_value = token_padding_value\n",
    "        self.embedding = Embedding(num_embeddings, embedding_dim, padding_idx=token_padding_value, max_norm=max_norm)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        self.q_linear = Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size)\n",
    "        self.k_linear = Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size)\n",
    "        self.v_linear = Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size)\n",
    "        self.multihead_attn = MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads)\n",
    "        self.head = Linear(hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, tokens: torch.LongTensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applying neural network layers to input 'tokens'.\n",
    "\n",
    "        Args:\n",
    "            tokens: the input tensor with tokens ids (batch_size, sequence_len)\n",
    "\n",
    "        Returns:\n",
    "            logits: the scores issued by the model (batch_size, num_classes, sequence_len)\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens)\n",
    "\n",
    "        # Используем специальную функцию pack_padded_sequence для того, чтобы получить\n",
    "        # структуру PackedSequence, которая не учитывать паддинг при проходе rnn.\n",
    "        # lengths -- длины исходных исходных последовательностей в батче,\n",
    "        # без учёта сдвига\n",
    "        lengths = (tokens != self.token_padding_value).sum(dim=1).detach().cpu()\n",
    "        packed_embed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            input=embed,\n",
    "            lengths=lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # Используем специальную функцию pad_packed_sequence для того, чтобы получить\n",
    "        # тензор из PackedSequence. Операция является обратной к pack_padded_sequence\n",
    "        packed_rnn_output, _ = self.rnn(packed_embed)\n",
    "        rnn_output, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=packed_rnn_output,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # ### START CODE HERE ###\n",
    "        query = self.q_linear(rnn_output)\n",
    "        key = self.k_linear(rnn_output)\n",
    "        value = self.v_linear(rnn_output)\n",
    "\n",
    "        key_padding_mask = (tokens == self.token_padding_value)\n",
    "        attn_output, _ = self.multihead_attn(query, key, value, key_padding_mask=key_padding_mask)\n",
    "        # ### END CODE HERE ###\n",
    "\n",
    "        logits = self.head(attn_output)\n",
    "        logits = logits.transpose(1, 2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezh9kLTkQEL9"
   },
   "source": [
    "**Задание. Проведите эксперименты и побейте метрику из части 2.** **<font color='red'>(1 балл)</font>**\n",
    "\n",
    "P.S. Eсли качества увеличить не получилось, это нужно обосновать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sE1C1tzEQEL-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 10]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f402b92c5318439186de73d1c37bcd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop over train batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "expecting key_padding_mask shape of (50, 64), but got torch.Size([64, 50])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# ### START CODE HERE ###\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m train(n_epochs\u001b[38;5;241m=\u001b[39mn_epoch,\n\u001b[0;32m     67\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     68\u001b[0m         train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[0;32m     69\u001b[0m         valid_dataloader\u001b[38;5;241m=\u001b[39mvalid_dataloader,\n\u001b[0;32m     70\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     71\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m     72\u001b[0m         writer\u001b[38;5;241m=\u001b[39mwriter,\n\u001b[0;32m     73\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     74\u001b[0m         model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBiLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m train(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n",
      "Cell \u001b[1;32mIn[44], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(n_epochs, model, train_dataloader, valid_dataloader, optimizer, criterion, writer, device, model_type)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     train_epoch(\n\u001b[0;32m     34\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     35\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[0;32m     36\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     37\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m     38\u001b[0m         writer\u001b[38;5;241m=\u001b[39mwriter,\n\u001b[0;32m     39\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     40\u001b[0m         epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m     41\u001b[0m         model_type\u001b[38;5;241m=\u001b[39mmodel_type,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[0;32m     43\u001b[0m     evaluate_epoch(\n\u001b[0;32m     44\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     45\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mvalid_dataloader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m         model_type\u001b[38;5;241m=\u001b[39mmodel_type,\n\u001b[0;32m     51\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[42], line 44\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, writer, device, epoch, model_type)\u001b[0m\n\u001b[0;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBiLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 44\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(tokens)\n\u001b[0;32m     45\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m logits\n\u001b[0;32m     46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n",
      "Cell \u001b[1;32mIn[74], line 72\u001b[0m, in \u001b[0;36mBiLSTMAttn.forward\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     69\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_linear(rnn_output)\n\u001b[0;32m     71\u001b[0m key_padding_mask \u001b[38;5;241m=\u001b[39m (tokens \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_padding_value)\n\u001b[1;32m---> 72\u001b[0m attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultihead_attn(query, key, value, key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# ### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     75\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(attn_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1343\u001b[0m         query,\n\u001b[0;32m   1344\u001b[0m         key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1369\u001b[0m         query,\n\u001b[0;32m   1370\u001b[0m         key,\n\u001b[0;32m   1371\u001b[0m         value,\n\u001b[0;32m   1372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[0;32m   1373\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight,\n\u001b[0;32m   1375\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k,\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v,\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[0;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1382\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m   1383\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m   1384\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[0;32m   1385\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   1386\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1387\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1388\u001b[0m     )\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:6206\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   6204\u001b[0m \u001b[38;5;66;03m# merge key padding and attention masks\u001b[39;00m\n\u001b[0;32m   6205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 6206\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m key_padding_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\n\u001b[0;32m   6207\u001b[0m         bsz,\n\u001b[0;32m   6208\u001b[0m         src_len,\n\u001b[0;32m   6209\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting key_padding_mask shape of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39msrc_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey_padding_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6210\u001b[0m     key_padding_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   6211\u001b[0m         key_padding_mask\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, src_len)\n\u001b[0;32m   6212\u001b[0m         \u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   6213\u001b[0m         \u001b[38;5;241m.\u001b[39mreshape(bsz \u001b[38;5;241m*\u001b[39m num_heads, \u001b[38;5;241m1\u001b[39m, src_len)\n\u001b[0;32m   6214\u001b[0m     )\n\u001b[0;32m   6215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: expecting key_padding_mask shape of (50, 64), but got torch.Size([64, 50])"
     ]
    }
   ],
   "source": [
    "train_dataset = NERDataset(\n",
    "    token_seq=train_token_seq,\n",
    "    label_seq=train_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "valid_dataset = NERDataset(\n",
    "    token_seq=valid_token_seq,\n",
    "    label_seq=valid_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "test_dataset = NERDataset(\n",
    "    token_seq=test_token_seq,\n",
    "    label_seq=test_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "\n",
    "collator = NERCollator(\n",
    "    token_padding_value=token2idx[\"<PAD>\"],\n",
    "    label_padding_value=-1,\n",
    ")\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "# ### START CODE HERE ###\n",
    "n_epoch = 10\n",
    "lr = 1e-4\n",
    "max_norm = 1.0\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "model = BiLSTMAttn(\n",
    "    num_embeddings=len(token2idx),\n",
    "    embedding_dim=300,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.0,\n",
    "    bidirectional=True,\n",
    "    n_classes=len(label2idx),\n",
    "    token_padding_value=token2idx[\"<PAD>\"],\n",
    "    max_norm=None,\n",
    "    num_heads=8,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98))\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "# ### START CODE HERE ###\n",
    "train(n_epochs=n_epoch,\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        valid_dataloader=valid_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        model_type=\"BiLSTM\")\n",
    "\n",
    "train(...)\n",
    "# ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvESUF7FVAjD"
   },
   "outputs": [],
   "source": [
    "evaluate_epoch(\n",
    "    model=model,\n",
    "    dataloader=test_dataloader,\n",
    "    criterion=criterion,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    epoch=10,\n",
    "    model_type='BiLSTM'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpMp9p8xcoeG"
   },
   "source": [
    "## Бонус 2: ChatGPT-теггер (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffRpFB-ZbRsr"
   },
   "source": [
    "Творческое задание, в котором Вам требуется использовать ChatGPT от OpenAI для разметки именованных сущностей. В этой части Вы вольны использовать любые приемы и ухищрения, чтобы заставить модель классифицировать токены.\n",
    "\n",
    "Ваше задание заключается в следующем:\n",
    "- с помощью ChatGPT разметить первые 30 объектов из test_token_seq;\n",
    "- на размеченных объектах посчитать качество с помощью ранее описанной функции `compute_metrics`;\n",
    "- написать выводы.\n",
    "\n",
    "Один из возможных вариантов, но не единственный  -- использование техники Few-Shot Learning. Суть заключается в том, что модели нужно скормить на вход \"правила игры\", то есть то, что мы будем подавать на вход и что мы ожидаем на выходе. Например:\n",
    "\n",
    "\"eu rejects german call to boycott british lamb . -> B-ORG O B-MISC O O O B-MISC O O\"\n",
    "\n",
    "\"peter blackburn -> B-PER I-PER\"\n",
    "\n",
    "\"the european commission said on thursday it disagreed with german advice to consumers to shun british lamb until scientists determine whether mad cow disease can be transmitted to sheep . -> O B-ORG I-ORG O O O O O O B-MISC O O O O O B-MISC O O O O O O O O O O O O O O\"\n",
    "\n",
    "\"my name is lomonosov ->\" (и тут просим модель выдать ответ).\n",
    "\n",
    "Так делаем для первых 30 последовательностей из тестовой части и считаем метрики качества (размера батча при этом также равен одному).\n",
    "\n",
    "Здесь есть несколько нюансов, которые мы раскрывать не будем. Вам предстоит столкнуться с ними в процессе. Также мы намеренно не предоставляем дополнительных подсказок, предлагая полную свободу действий. Любые нестандартные техники и идеи приветствуются и будут поощераться баллами.\n",
    "\n",
    "**Важные детали**:\n",
    "- Вам нужно зарегистрировать аккаунт в системе OpenAI, лучше всего делать это через Gmail (домен @mail.ru, например, банится и не регистрируется).\n",
    "- Также Вам понадобиться VPN, без него по некоторым причинам не получится зайти на сайт, зарегистрироваться и воспользоваться моделью.\n",
    "- У Вас есть лимит на количество токенов, которые Вы можете обработать, поэтому расходуйте ресурс разумно. Но Вы можете регистрировать несколько аккаутов, пополнять баланс, использовать более \"дорогие\" модели -- здесь на Ваш выбор.\n",
    "- Основной целью этой части задания является показать, что LLM также можно использовать разметке именнованных сущностей. Так как техник очень много, мы предлагаем ориентироваться на порог качества **0.70** и выше по `f1-macro`. Этот порог можно достичь на стандартной версии `gpt-3.5-turbo`, без дополнительных денежных трат, ограничиваясь бесплатным лимитом.\n",
    "- Напишите содержательный вывод и Ваше мнение о целесобразности такого подхода, в чем его преимущества и недостатки, в каких ситуациях он имеет место быть, а где лучше использовать стандартные LSTM/Transformer-модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqcQWCuz2ZnH"
   },
   "outputs": [],
   "source": [
    "!pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opn_4LeNTryx"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xuGUfPtPpKs"
   },
   "outputs": [],
   "source": [
    "# ### START CODE HERE ###\n",
    "...\n",
    "# ### END CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4MIrbmNoQEMA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d4ce941904148077feb793883e611d25d231ca995d9164b22ee99fd0facd8d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
